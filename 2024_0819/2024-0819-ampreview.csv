headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
マイクロソフト「MInference」はオンプレでの推論スピードを10倍アップ、クラウドに依存しない生成AI利用を促進する技術開発が加速（AMP［アンプ］）,https://news.yahoo.co.jp/articles/48c785d0d955ac8e81d2259441d6357658cd6dd9,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240819-00010000-ampreview-000-1-view.jpg?exp=10800,2024-08-19T06:01:29+09:00,2024-08-19T06:01:29+09:00,AMP［アンプ］,ampreview,AMP［アンプ］,2938,"\n生成AIをめぐる大きなトレンド、小型オープンモデルをオンプレで利用\nマイクロソフト「MInference」\nコンシューマと企業、ともに生成AI利用が拡大しているが、それぞれに異なるトレンドが形成されている。\n\nコンシューマ側では、ChatGPTやClaudeなどチャットツールに加え、これらの開発企業が提供するAPIをベースとするサードパーティツールの利用が広がりを見せる。一方、企業においては、同じくChatGPTやAPIの利用が散見されるものの、扱う情報の機密性が高い場合、情報漏洩などの懸念から、カスタマイズしたオープンソースモデルをオンプレミスで利用するケースが増えているのだ。\n\nこうした企業の需要を反映する形で、小型言語モデル（SLM）に関連する動きが活発化している。たとえば、小型言語モデルの開発に特化したAIスタートアップAcree AIは、2024年1月に500万ドルのシード資金を調達したばかりだが、2024年7月にシリーズAの資金調達を実施、2,400万ドルを獲得した。\n\n同社は、特定分野に特化した小型モデルの開発を強みとしており、短期間で、人事、税務、教育、医療分野における専門小型モデルの開発を担ってきた実績を持つ。これらの小型モデルは、70億パラメータほどでうまく機能するモデルであるという。ChatGPTのGPT-3.5のベースとなっているGPT-3️のパラメータ数1,750億と比較すると、サイズは25分の1だ。\n\n2024年7月18日、NVIDIAとフランスのスタートアップMistral AIが共同で発表した「Mistral-NeMo」も小型言語モデルトレンドの強まりを示す事例といえるだろう。\n\nこのモデルは120億パラメータを持つ比較的小さなモデルで、クラウドではなく、企業のデスクトップでの利用が想定されている。特徴は、グーグルのGemma2️ 9B（90億パラメータ）やメタのLlama3 8B（80億パラメータ）など同サイズのモデルと比べて、圧倒的に多いコンテキストウィンドウだ。コンテキストウィンドウとは、プロンプトに入力できるデータの量。英語であれば100トークン、75ワードほどに換算される。\n\n前者2モデルのコンテキストウィンドウがそれぞれ8,000トークンであるのに対し、Mistral NeMoは12万8,000トークンを誇るのだ。企業はクラウドに接続することなく、自社のオフラインPCでセキュアに、大量の文章を読み込ませ、さまざまなタスクを遂行できるようになると期待される。\nマイクロソフトのMInferenceアプローチ、その特徴とは\nNVIDIAのDGXステーション（https://docs.nvidia.com/dgx/dgx-station-a100-user-guide/intro-to-station-a100.html\nこれらのモデルをオンプレミスで利用する際、必須となるのがGPUだ。一般的にはNVIDIAのGPUが利用される。仮に社員100人規模の企業がオンプレミスで生成AIモデルのスムーズな運用を行う場合、NVIDIAのA100GPUが6～8台ほど必要になると想定される。\n\n問題となるのは、そのコスト。8台のGPUを利用する場合、並列処理などが最適化された「DGX」ステーションの形での利用が推奨されるが、そのコストは、導入費用だけでも少なくとも20万ドル（3,150万円）に上る。ここに電力、保守などの運用費用が加算されることになり、年間コストは非常に高額なものになってしまうのだ。\n\nマイクロソフトがこのほど発表した「MInference」は、このコストを大幅に下げる可能性を秘めており、AIコミュニティでも注目される存在となっている。\n\nこの技術は、言語モデルの処理において大きなボトルネックとなっている「プリフィリング」段階を大幅に高速化するもので、100万トークン（約700ページ分のテキスト）の入力に対して、処理時間を最大90%削減することができるという。\n\nこの手法により、マイクロソフトの研究チームは80億パラメータのモデルで100万トークンを処理するのに、従来は1台のNVIDIA A100 GPUで30分かかっていたところを、約3分まで短縮、最大10倍の高速化が可能になったと報告している。\n\nMInferenceの特筆すべき点は、既存の言語モデルに対して追加の学習や微調整を必要とせず、直接適用できることだ。これにより、モデルの精度を維持しつつ、処理速度を大幅に向上させることが可能となる。また、この技術はGPUの特性を考慮して最適化されており、NVIDIA A100 GPUでの性能向上が報告されているが、他のデバイスへの移植も容易であるという。\n\n上記の条件であれば、同じパフォーマンスを発揮するのに必要なGPUは半分となり、コストを大幅に下げることが可能だ。特に予算が限定される中小企業や研究機関にとって、ゲームチェンジャーとなるアプローチになるかもしれない。\n今後想定されるユースケース\nMInferenceのような手法が広く実用化されると、低コストで多様な長文処理タスクに対応できるようになり、企業におけるAI普及を後押しすることになるだろう。\n\n冒頭でも触れたように、これまでのオープンソースモデルは、コンテキストウィンドウのサイズが8,000トークンと小さなものだったが、12万8,000トークンのMistral NeMoの登場により、今後、より大きなコンテキストウィンドウを持つオープンソースモデルの開発が加速する公算が高まっている。しかし、オンプレミスマシンの処理能力が低い場合、これらのモデルの能力を十二分に発揮することは難しい。ここにMInferenceのような手法を導入することで、限られたAIリソースでも、AIモデルのパフォーマンスを高めることが可能になるのだ。\n\nそうなると、これまでのオープンソースモデルでは難しかった文書分析や要約タスクにおいて大きな進展が期待できる。大型コンテキストウィンドウモデルとMInferenceによる高速化により、たとえば、法律文書、学術論文、長編小説などの大規模テキストデータの分析や要約が格段に容易になる。複雑な契約書の自動レビューや、大量の研究論文からのトレンド分析などが、より迅速かつ低コストで実現できるかもしれない。\n\nまた高度な質問応答システムの構築も不可能でない。AIモデルに、特定トピックに関する膨大な情報を与えることで、チャットボットの回答精度を大きく改善することができる。医療分野での診断支援や、法律相談、複雑な技術的問い合わせへの対応など、専門知識を要する領域で特に有用になると考えられる。\n\nこのほか、数十万行に及ぶソースコードを一度に処理し、バグの検出や最適化の提案、さらには大規模なリファクタリングの支援などコーディング／ソフトウェア開発でも大きな躍進が期待できる。\n文：細谷元（Livit）","['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240819-00010000-ampreview-000-1-view.jpg?pri=l&w=640&h=511&exp=10800', 'https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240819-00010000-ampreview-001-1-view.jpg?pri=l&w=640&h=329&exp=10800']","['https://news.yahoo.co.jp/articles/48c785d0d955ac8e81d2259441d6357658cd6dd9/images/000', 'https://news.yahoo.co.jp/articles/48c785d0d955ac8e81d2259441d6357658cd6dd9/images/001']"
