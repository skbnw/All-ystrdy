headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
SBIグループとPFN、次世代AI半導体開発などで提携（EE Times Japan）,https://news.yahoo.co.jp/articles/29598e44853b7759300bf4add65ee2d933a1b8a9,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000094-it_eetimes-000-1-view.jpg?exp=10800,2024-09-02T17:01:04+09:00,2024-09-02T17:01:04+09:00,EE Times Japan,it_eetimes,EE Times Japan,897,\n資本業務提携の概要［クリックで拡大］ 出資：SBIホールディングス他\n次世代AI半導体製造プロセスの後工程でも連携\n\n SBIホールディングスとPreferred Networks（PFN）は2024年8月27日、次世代AI半導体の開発および製品化に向け、資本業務提携を行うことで基本合意したと発表した。この合意に基づき、SBIホールディングスはPFNに対し、2024年9月末にも最大100億円規模の出資を行う予定。\n\n PFNは、AI技術の実用化に向け、ハードウェアからソフトウェアまでを垂直統合で開発し提供するスタートアップで、2014年に創業した。AI半導体の設計や周辺ソフトウェアの開発、自社製のAIプロセッサ「MN-Coreシリーズ」を搭載したスーパーコンピュータの開発、さらには生成AI基盤モデルの構築および、これらを応用したアプリケーションを開発する。\n\n また、MN-Coreシリーズを用いた計算基盤や、SaaSによる大規模プラントの自動運転、材料開発向け原子レベルシミュレーションなども提供している。なお、スーパーコンピュータの電力効率ランキング「Green500」では、第1世代のMN-Coreを搭載したスーパーコンピュータが、2020年と2021年に世界1位を3度も獲得したという。\n\n SBIホールディングスは今回、PFNが開発する次世代AI半導体の社会実装を強力にサポートしていくため、資本業務提携を行うことにした。具体的には、「PFNの次世代AI半導体の製品化に向けた共同研究ならびに開発」「PFNの次世代AI半導体の製造プロセスにおける後工程の連携」そして、「PFNの資金調達等のファイナンス協力」などを検討していく。\n\n 半導体事業に関してSBIグループは、これまで「JSMCホールディングス」を設立。半導体大手商社のレスターと業務提携したり、宮城県大衡村で事業予定用地を確保したりして、半導体事業の拡大に取り組んできた。新たにPFNの次世代AI半導体を製造することで、製造業としての認知度を高めていく。\nEE Times Japan,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000094-it_eetimes-000-1-view.jpg?exp=10800'],['https://news.yahoo.co.jp/articles/29598e44853b7759300bf4add65ee2d933a1b8a9/images/000']
「超垂直な」メモリホールを高速加工　1000層NANDの実現に向け（EE Times Japan）,https://news.yahoo.co.jp/articles/d4c45760a76e9bb2e574aa6f0041709347825b53,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000092-it_eetimes-000-1-view.jpg?exp=10800,2024-09-02T17:00:02+09:00,2024-09-02T17:00:02+09:00,EE Times Japan,it_eetimes,EE Times Japan,1983,\n（写真：EE Times Japan）\nLam Research（以下、Lam）は2024年7月、第3世代となる極低温絶縁膜エッチング技術「Lam Cryo（クライオ） 3.0」を発表した。これに伴い、日本法人のラムリサーチは同年8月22日、記者説明会を開催し、同技術の詳細を説明した。Lamのエッチングチャンバー「Vantex」に適用することで、3D（3次元） NAND型フラッシュメモリのメモリホールを高速かつ高精度に加工できるようになる。1000層を超える3D NANDフラッシュの開発が進む中、その実現を支える重要な技術になると、ラムリサーチ Regional Technology Group Managing Directorの西澤孝則氏は強調する。\n積層数が増えると、メモリホールの加工の難易度が上がる［クリックで拡大］ 出所：Lam Research\n「深くて真っすぐな穴を掘る」\nAI（人工知能）の普及などで最先端メモリ／ストレージの需要が増大していることから、NANDフラッシュでは高密度化によるビットコスト（ビット当たりのコスト）の低減が、以前にも増して求められるようになっている。そのため、特に積層化の競争は激しく、「NANDフラッシュメーカー各社は、1000層の3D NANDフラッシュの開発を視野に入れている」（西澤氏）\n\n 現在、NANDフラッシュでは200層や300層を超える積層数の製品が発表されている。このように積層数が多いNANDフラッシュの製造では、メモリホール（メモリチャネル）をいかに高精度かつ高速に加工できるかが重要になる。「200層を超えるNANDフラッシュでは、メモリホールのアスペクト比は50：1を超える。しかも、1枚の300mmウエハーに1兆個以上のメモリホールを、ほぼ完璧に開けなくてはならない」（西澤氏）\n\n これだけアスペクト比が高い穴を加工するのは難易度が高い。まず、そもそも「垂直に」加工することが難しい。途中でゆがんだり、下層に行くにつれて細くなってしまったりするからだ。メモリホールの垂直性が不十分だとデバイス性能に悪影響を与える。垂直に加工できず、メモリホールの穴径（CD：Critical Dimension））が上層と下層で違い過ぎると、多値メモリのしきい値判別などに支障が出てしまう。「メモリセルの積層数を増やすためには、メモリホールを上から下まで垂直に加工できる技術が必要になる」（西澤氏）\n\n Lamは、高アスペクト比（HAR：High Aspect Ratio）のメモリホールを高精度に加工するHAR絶縁体エッチングチャンバーや技術を提供してきた。Cryoは、極低温でエッチングすることで、垂直性が高く、かつ真円度が高い（上から見ると真円に近い）メモリホールを加工できる技術だ。2019年に第1世代をリリースし、量産に適用した。現在、LamのHAR絶縁体エッチングチャンバーは7500台以上がNANDフラッシュの製造に使われていて、そのうち約1000台にCryoが適用されているという。西澤氏は「当社の極低温エッチング技術は、安定した性能を量産において実現できていることが最大の強みだ」と強調する。\nΔCDは9nm 垂直な穴を高速に加工\n新たにリリースしたCryo 3.0では、最大10μmの深さで加工できることに加え、エッチング速度は常温プロセスに比べて2.5倍になる。メモリホール最上部のCDが108nm、最下部のCDは99nmで、両者の差（ΔCD）は9nmと極めて小さいことも特徴だ。常温プロセス適用時では、ΔCDは30nm程度だという。つまり、アスペクト比が高い穴を“真っすぐに”加工できるということだ。「Cryo 3.0ではアスペクト比100：1の穴を、極めて垂直かつ高速に加工できるようになる」（西澤氏）\n\n さらに、Cryo 3.0では新しい成分のエッチングガスを採用したことで、従来の常温エッチングプロセスと比較してウエハー1枚当たりの電力消費量を40％削減し、CO2排出量も最大90％削減するという。\n\n Cryo 3.0に対応するチャンバーとして「Vantex Cシリーズ」をリリース済みだ。Cryo 1.0に対応する「Flex Hシリーズ」やCryo 2.0に対応する「Vantex Bシリーズ」など既存のチャンバーにもCryo 3.0を適用できる。\n\n 現在、Cryo 3.0は量産に向けた技術評価を行っている段階だ。「ビットコストを下げる手段には複数存在するが、高アスペクト比の穴をいかに垂直に加工できるかという点は、ビットコスト低減において最も象徴的な技術の一つではないか」（西澤氏）\nEE Times Japan,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000092-it_eetimes-000-1-view.jpg?pri=l&w=640&h=334&exp=10800'],"['https://news.yahoo.co.jp/articles/d4c45760a76e9bb2e574aa6f0041709347825b53/images/000', 'https://image.itmedia.co.jp/l/im/ee/articles/2409/02/l_mm240903_lam02.jpg#utm_source=yahoo_v3&utm_medium=feed&utm_campaign=20240902-092&utm_term=it_eetimes-sci&utm_content=img']"
GPSなしで太平洋を自律飛行も　東芝の慣性センサーモジュール（EE Times Japan）,https://news.yahoo.co.jp/articles/0e0d120beed1636400a99b320a85d471c394987f,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000093-it_eetimes-000-1-view.jpg?exp=10800,2024-09-02T17:00:28+09:00,2024-09-02T17:00:28+09:00,EE Times Japan,it_eetimes,EE Times Japan,1571,\n慣性センサーの概要［クリックで拡大］ 出所：東芝\n東芝は2024年9月、MEMS技術を用いて小型化し、同時に世界最高レベルの精度を実現した「慣性センサーモジュール」を開発したと発表した。このモジュールの精度は、航空機に搭載して太平洋航路をGPSなしで自律飛行できるレベルだという。また、東芝電波プロダクツは、新開発のジャイロセンサーを用い、小型の「可搬型ジャイロコンパス」を開発した。\n開発したMEMS慣性センサーの概要［クリックで拡大］ 出所：東芝\n慣性センサーは、加速度センサーとジャイロセンサーで構成される。加速度センサーは物体が縦横に動く「並進運動」を、ジャイロセンサーは物体の「回転運動」を、それぞれ計測する。物体の移動量と回転量が分かれば、その動きや位置を求めることができるという。\n\n つまり、慣性センサーを用いるとその出力だけで、自分の位置を割り出すことが可能であり、あらゆる環境で測位できるという特長がある。GPSはビル陰やトンネル内など電波が届かない場所では機能しない。カメラで撮影すると暗い場所では鮮明な画像が取得できない、といった課題も解決できる。\n\n 慣性センサーは既に、スマートフォンなどで利用されている。MEMS技術により小型化を実現しているが、スマホ向け慣性センサーは高い精度が期待できない。一方で、航空機や防衛システム、産業機器などに搭載される機械式／光学式慣性センサーは、高い精度が得られるものの、サイズは最小でも数十センチメートル角以上と大きくなっていた。\n\n そこで東芝は、「ナビゲーショングレード」と呼ばれる機械式／光学式と同等精度を実現しながら、独自のMEMS技術を駆使してサイズが数センチメートル角と小さいMEMS慣性センサーの開発に取り組んだ。\n\n 新たに開発したのは「フーコーの振り子」の原理で角度を検出する「MEMS角度直接検出型ジャイロセンサー（MEMS-RIG）」と、共振はりの周波数変化で慣性力を検出する「MEMS差動共振型加速度センサー（MEMS-DRA）」である。\n\n 従来のMEMS慣性センサーは物理量を「重りの変位」で検出していた。新たに開発したMEMS慣性センサーは、動き検出に「変位」を用いず、振動子の方向や周波数で行う。これにより、高い精度と高ダイナミックレンジ（DR）を両立させた。\n\n 試作した慣性センサーモジュールの大きさは約12×8×2cmで200cc弱だが、開発目標としては10cc程度を目指している。バイアス安定性（BI）の目標は、ジャイロセンサーが0.01dph以下、加速度センサーが1μG以下と設定している。開発品の「アラン分散」を見ると、ジャイロセンサーのBIは0.01dphと同等かそれ以下、加速度センサーのBIは0.43μGと同等かそれ以下となった。\n可搬型ジャイロコンパス、0.056度の方位角精度で「真北」を推定\n東芝電波プロダクツは、東芝が開発したジャイロセンサーを用い、可搬型のジャイロコンパスを開発した。持ち運び可能なサイズを実現しながら、0.056度の方位角精度で「真北」を推定することに成功した。この方位角精度は防衛用レーダーなどに用いられる機械式・光学式並みの精度である。ちなみに、0.056度の方位角精度は1km先で1mの誤差となる。これに対し、従来のMEMS式ジャイロセンサーを用いた方位角精度は約1度である。この方位角精度だと1km先では18mの誤差が生じるという。\n\n 開発したジャイロコンパスの大きさは約4リットル（直径180mm、高さ161.6mm）だが、「バッテリーを工夫すればもう少し小型化できる」という。2026年度にも製品化の予定である。\nEE Times Japan,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000093-it_eetimes-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/0e0d120beed1636400a99b320a85d471c394987f/images/000', 'https://image.itmedia.co.jp/l/im/ee/articles/2409/02/l_tm_240902toshiba02.jpg#utm_source=yahoo_v3&utm_medium=feed&utm_campaign=20240902-093&utm_term=it_eetimes-sci&utm_content=img']"
推論性能でNVIDIAに挑む　AIチップは「省エネ」が競争の軸に（EE Times Japan）,https://news.yahoo.co.jp/articles/c7943ea7f489d9d844bd69de43ef13eec1c47dae,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000091-it_eetimes-000-1-view.jpg?exp=10800,2024-09-02T16:59:34+09:00,2024-09-02T16:59:34+09:00,EE Times Japan,it_eetimes,EE Times Japan,4993,\nAMDの「MI300X」のスコア［クリックで拡大］ 出所：AMD\nAMDは、推論ベンチマーク「MLPerf」の最新ラウンドに、同社のデータセンター用GPUのフラグシップ製品である「AMD Instinct MI300X（以下、MI300X）」の結果を初めて提出した。同社は、市場リーダーであるNVIDIAの現世代ハードウェア「H100／H200」に匹敵する結果を達成したが、総合的にはNVIDIAがわずかな差で勝利した。\nNVIDIAのスコア［クリックで拡大］ 出所：NVIDIA\nNVIDIAは、カナダのスタートアップ企業であるUntetherからも挑戦を受けた。Untetherは同社初となるMLPerfベンチマークを披露し、同社の「speedAI」アクセラレーターが「ResNet-50」ワークロードの電力効率において、NVIDIAのさまざまなチップを上回ったことを示した。GoogleもAI向けアクセラレーターチップ「Tensor Processing Unit（TPU）」の第6世代となる「Trillium」の結果を提出し、Intelは「Granite Rapids CPU」を初披露した。\nAMDは「MI300X」のスコアを初めて提出\nAMDは、NVIDIAのデータセンター向けGPUに対抗するMI300Xの結果を初提出し、「Llama2-70B」での推論におけるシングルチップおよび8チップシステムの性能を示した。シングルチップのMI300Xは、サーバモードで2520.27トークン/秒（token/s）、オフラインモードで3062.72トークン/秒の推論が可能で、8チップシステムのMI300Xは、サーバモードで21028.20トークン/秒、オフラインモードで23514.80トークン/秒を実行できる。この数字は、システムサイズ間のスケーラビリティがかなり直線的であることを示している（オフラインシナリオではバッチ処理によってスループットを最大化できるが、より難しいサーバシナリオではレイテンシの制限に対応したリアルタイムクエリをシミュレートする）\n\n この結果は、NVIDIAの8チップシステムH100（80GB）で同じワークロードを処理した場合の結果と非常によく似ている（差は3～4％）。NVIDIAのH200（141GB）（実質的に、より多くの高速メモリを搭載したH100）と比較すると、AMDは30～40％遅くなっている。\n\n AMDは、12チップレットのMI300X GPUをNVIDIAのH100に直接競合する製品と置付けていて、これはNVIDIAの市場独占に挑む最も有望な商用製品の一つとして広く期待されている。MI300Xは、NVIDIAのH100およびH200よりもHBM（広帯域幅メモリ）の容量と帯域幅が大きく（H200が141GB（ギガバイト）で4.8Tバイト/秒（TB/s）であるのに対して、MI300Xは192GBで5.2TB/s）、その差は大規模言語モデル（LLM）での推論結果に顕著に表れると思われる。AMDは、「192GBは1チップでLlama2-70Bモデル全体とKV（Key Value）キャッシュを保持するのに十分な容量で、これにより、モデルを複数のGPUに分割することによるネットワークオーバーヘッドを回避できる」と述べている。さらに、「MI300Xは、H100／H200よりもわずかにFLOPSが高い。H100と同等で、H200に後れを取っていることは、AMDファンを少し失望させるかもしれないが、これらの初期スコアは、ソフトウェアのさらなる最適化によって、次のラウンドで間違いなく改善される」とも説明した。\n\n ソフトウェア面では、AMDは、「コンポーザブルカーネルライブラリを広範囲に使用して、プレフィルアテンションやFP8デコードページドアテンション、さまざまな融合カーネル向けにパフォーマンスクリティカルなカーネルを作成した」と述べている。また、デコードスケジューリングの高速化とプレフィルバッチ処理の改善のためにスケジューラも改善したという。\n\n AMDはMI300Xと共に、次世代（第5世代）「EPYC」プロセッサである「Turin」も披露した。第4世代EPYCプロセッサである「Genoa」を搭載した同一システムと比較して、サーバモードで4.7％、オフラインモードで2.5％と性能改善はわずかではあるが、TurinベースのシステムをDGX-H100よりも若干高速化することができた。AMDのTurin CPUはまだ市場に投入されていない。\nNVIDIA「Blackwell B200」\nNVIDIAはこのラウンドで「Blackwell B200」を初公開した。Blackwell B200は、新しいBlackwellアーキテクチャを搭載した初のGPUで、レチクルサイズのコンピューティングダイを2つ搭載しているため、H100／H200の2倍のコンピューティング能力がある。また、H200の141GBに対して180GBとより多くのメモリを搭載している。さらに、FP4をサポートするNVIDIA初のGPUでもある。\n\n サーバモードの単一のB200は、Llama2-70Bdで10755.60トークン/秒の推論が可能で、H100よりも最大で4倍高速である。オフラインモードでは11264.40トークン/秒で、H100よりも最大3.7倍速い。注目すべきは、NVIDIAが今回の結果で初めてFP4に量子化したことだ（提出者は、厳密な精度目標（今回は99.9％）を満たすことを条件に、希望に応じて積極的に量子化することができる）。H200（H100よりも大容量で高速なメモリを搭載し、FP8に量子化）と比較すると、どちらのシナリオでも2.5倍以上の差があった。NVIDIAは、ワークロード全体をFP4に量子化したかどうかについては言及していない。同社のTransformerエンジンソフトウェアは、トレーニングと推論中に最適な結果を得るために混合精度を実現している。\n\n B200の結果は「プレビュー」カテゴリーに提出された。つまり、NVIDIAはBlackwellを6カ月以内に市場投入すると予想される。両社の製品サイクルから、AMDの現世代MI300Xは、近いうちにNVIDIAのB200に匹敵する性能になると予想される。Llama2-70Bの推論では、B200は現在のMI300Xより約4倍高速である。\n\n NVIDIAと同社のパートナーは、新しいエキスパートワークロードの組み合わせである「Mixtral-8x7B」の結果を披露した。このワークロードは、総パラメータ数が467億個で、トークンあたり12.9アクティブである（この場合、エキスパートモデルの組み合わせは、「エキスパート」と呼ばれる8つの小さなサブモデルのいずれかにクエリを割り当てる）。Mixtralでは、同じ電力エンベロープ（700W）でH200がH100を上回った。H200はメモリが1.8倍、帯域幅が1.4倍多く、性能は約11～12％向上している。\n\n NVIDIAは、「前回の推論ラウンドと比較して、『Hopper』GPUから27％高い性能を引き出しており、この成果の多くは当社の開発者ツールに組み込まれている」と述べている。\nカナダ新興のUntether\nUntetherは、同社の第2世代アクセラレーター「speedAI240」の性能と電力のスコアを複数の異なるシステム構成で測定した、初のMLPerf結果を発表した。speedAI240は、電力効率に優れたAI推論向けに設計された、1400以上のRISC-Vコアを搭載する2-PFLOPSアクセラレーターである。\n\n Untetherの6チップ構成の「Slim」PCIeカードは、それぞれ75Wの電力で動作するアクセラレーター1個を搭載し、2Uに収まる。ResNet-50 v1.5のベンチマークでは、サーバモードで30万9752クエリ/秒を処理できる。これは、米国のSuper Micro Computer（以下、Supermicro）が提出したNVIDIA 100-SXM-80GBの8チップ搭載システムの約半分の性能だが、Supermicroのシステムは2倍大きい4Uで、TDP（熱設計電力）は10倍以上である。アクセラレーターごとに正規化すると、この構成では、1つのUntether speedAI240の性能はH100の約65％に相当する。ただし、UntetherはHBMを使用していない。Untetherのアクセラレーターは、帯域幅100GB/秒で最大64GBのLPDDR5メモリを搭載している。\n\n Untetherは、プレビューカテゴリーの（つまり、まだ市場に出回っていない）システムの結果も提出した。同システムは、より大きなシングルチップPCIeカードをベースとしているため、アクセラレーターが利用可能な電力は2倍の150Wで、クロック周波数もわずかに向上している。ResNet-50のベンチマーク結果は、アクセラレーターごとに、サーバモードで35％、オフラインモードで26％向上した。2枚のカードを組み合わせると、性能が2倍になり、線形スケーラビリティが実証された。\n\n Untetherは、電力効率において非常に優れた結果を示している。ResNet-50の場合、スリムカード（各75W）の6個のアクセラレーターは、サーバモードでワット当たり314クエリ/秒を推論できるのに対し、NVIDIAのH200-141GB 8個では96クエリ/秒で、UntetherはNVIDIAの現世代のハードウェアと比較して約3倍の電力効率を実現している。\n\n Untetherは、「次のラウンドでは、4アクセラレーターカードのベンチマークを実施し、Googleの自然言語処理モデル『BERT』やより大きなLLMワークロードに挑戦する予定だ」と述べている。\nGoogleは「Trillium」のプレビュー結果を提出\nGoogleは、2024年後半に発売予定の次世代TPU v6eであるTrilliumの“プレビュー”結果を発表した。Trilliumは画像生成モデル「Stable Diffusion」を、サーバモードで4.49クエリ秒、オフラインモードで5.44サンプル/秒で推論できる。同じラウンドの現世代のTPU v5eと比較すると、性能は約3倍になる。NVIDIA GH200（「Grace Hopper」144GB）と比較すると、GH200はサーバモードで2.02クエリ/秒、オフラインモードで2.30サンプル/秒で、これはTrilliumの約半分の性能である。\n\n Googleは、「Trilliumは、より大きな行列乗算ユニットとより高速なクロック速度によって、前世代と比較してピーク演算性能が4.7倍向上する見通しだ」と述べている。HBMの容量と帯域幅も倍増し、カスタム光インターコネクトにより、チップ間の帯域幅も2倍になったという。\nIntelの「Granite Rapids」\nIntelはAI推論用CPUを披露した（今回はHabanaからの応募はなかった）。次世代「Xeon」サーバプロセッサである「Granite Rapids」（開発コード名）は、全てのパフォーマンスコア（Pコア、効率コアまたはEコア）を搭載している。Granite Rapidsは、前世代のXeon CPUの1.9倍の性能を提供する。これは、GPT-J（6B）までの小型モデルのみを含む、提出された全てのワークロードの平均値である。\n\n 同社は、新しいデータ型の導入やAMX（Advanced Matrix eXtensions）命令セット拡張の効率向上など、CPUロードマップにおいてAIへの投資を継続しているという。\n\n※米国EE Timesの記事を翻訳、編集しました。\nEE Times Japan,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240902-00000091-it_eetimes-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/c7943ea7f489d9d844bd69de43ef13eec1c47dae/images/000', 'https://image.itmedia.co.jp/l/im/ee/articles/2409/02/l_mm240902_ai02.jpg#utm_source=yahoo_v3&utm_medium=feed&utm_campaign=20240902-091&utm_term=it_eetimes-sci&utm_content=img']"
