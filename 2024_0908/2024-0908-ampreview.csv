headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
生成AI競争は次のステージへ、動画をリアルタイムに理解するAIの登場、OpenAIやグーグルの競争も激化（AMP［アンプ］）,https://news.yahoo.co.jp/articles/88c6b17c03d79e27d8d53e78d08396347f246b89,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240908-00010000-ampreview-000-1-view.jpg?exp=10800,2024-09-08T06:04:02+09:00,2024-09-08T06:04:02+09:00,AMP［アンプ］,ampreview,AMP［アンプ］,2827,\nOpenAIに対抗、グーグルのProject Astraとは？\nOpenAIに対抗、グーグルのProject Astraとは？\n生成AIの競争は次のステージに突入、OpenAIやグーグルなど主要企業による競争はさらに激化の様相となっている。\n\nOpenAIは2024年5月13日、新たな大規模言語モデル「GPT-4o」を発表した。GPT-4oは、テキスト、音声、ビジョンに対応できるマルチモーダル大規模言語モデル（LLM）だ。ユーザーがChatGPTスマートフォンアプリで撮影したリアルタイムのビデオを受け取り、分析することもできるようになる。\n\nこれに対抗するかのように、グーグルも翌14日の年次開発者会議「Google I/O」で、Project Astraを発表した。\n\nProject Astraは、同社の主要LLMの1つGemini Pro 1.5をベースに構築されたマルチモーダルAIエージェント。マルチモーダルエージェントとは、テキストだけでなく、音声やビジュアル情報にも対応できるAIシステムのことだ。GPT-4oと同様に、リアルタイムの動画を受け取り、応答することができる。\n\nグーグルが公開したデモ動画では、Pixelスマートフォン上で動作するプロトタイプのAstraエージェントが、カメラを通して物体を識別し、その特定のコンポーネントを説明し、ホワイトボードに書かれたコードを理解する様子が示された。具体的には、ユーザーがスマホを持ち、オフィス内を撮影しつつ、「この中で音を発するものがあれば教えて」と入力すると、パソコンの隣に設置されたスピーカーを指し、音を発するものだと回答。また、スピーカーの一部分を指し、その部分の名称を聞くと、しっかりと回答するシーンが披露された。\n\nまた別のデモ動画では、ホワイトボードに手書きされたデータベースに関するシステムアーキテクチャの図を認識し、サーバー間のキャッシュを増やすことでデータベースのスピードが高速化するなど、改善案を提示する様子も示された。\n\nグーグル・ディープマインドのデミス・ハサビスCEOは、マルチモーダルの推論では大きな進歩を遂げたが、エージェントの応答時間を人間の会話レベルまで短縮することが大きな課題だったと述べている。この課題を解決するため、グーグルは、ビデオフレームを連続的にエンコードし、ビデオと音声の入力を時系列に結合、この情報をキャッシュすることで、必要な情報を素早く処理する仕組みを構築したという。\n\nProject Astraの具体的な製品化時期は明らかにされていない。ただし、Android、iOS、ウェブ上のGeminiアプリに、同様の機能が実装される予定となっている。\nグーグルが意識するOpenAIのGPT-4oとは\nグーグルのProject Astraは、OpenAIが前日の5月13日に発表した「GPT-4o」を強く意識したものとなっている。\n\nGPT-4o（Omni）もテキストに加え、音声、ビジョンの情報を理解し、推論できるモデル。ユーザーがChatGPTスマートフォンアプリで撮影したリアルタイムの動画を受け取り、分析することも可能で、この発表時点から数週間以内にアプリで利用できるようになる予定だ（2024年6月7日時点では、まだ利用できない）。\n\n特筆すべきは、GPT-4oがリアルタイムで音声に応答する速度と、オーディオとビデオからユーザーの感情状態を検出し、それに応じて声を調整できる点だ。プレゼンテーション中のデモの1つでは、GPT-4oを搭載したChatGPTに、ドラマチックかつ演劇的な声でストーリーを語るよう求めたところ、素早く対応する様子が披露された。\n\nOpenAIによると、GPT-4oの反応速度は、人間の応答時間とほぼ同じで、最短で232ミリ秒、平均で320ミリ秒で応答できるという。GPT-4o以前は、音声モードを使った場合、GPT-3.5では平均2.8秒、GPT-4では5.4秒のレイテンシー（遅延）が発生し、スムーズな会話を行うことが難しかったが、反応速度の大幅な改善により、人間と会話しているかのようなUXを実現した。\n\n以前のモデルと根本的に異なる仕組みを導入したことがGPT-4oの高速性につながったという。\n\nGPT-4o以前の音声入力と出力では、3つの独立したモデルが稼働しており、以下のようなプロセスだった。\n\nユーザーが音声を入力すると、以下のプロセスが実行される。\n\n・1つ目のモデルは、オーディオからテキストを書き起こす。\n・2つ目のモデルがこのテキスト入力に対して、出力を生成する（質問に対する答えなど）。\n・3つ目のモデルが、出力されたテキストを音声に変換する。\n\nこのプロセスでは、応答速度が遅くなるだけでなく、話者のトーン、背景ノイズ、また複数話者の存在を直接捉えることができず、それに応じた感情表現を出力することも不可能だった。\n\n一方、GPT-4oでは、入力から出力までの一連の処理を単一のニューラルネットワークで行う方式を採用。テキスト、画像、音声に関わりなく、単一のモデルで対応することで、大幅な高速化を実現した。\n\nOpenAIは、GPT-4oを搭載したデスクトップアプリをまずMac向けにリリースし、その後ウィンドウズ向けのアプリを年内までにリリースする予定とのこと。\nメタもマルチモーダルの波に\nOpenAIやグーグルだけでなく、メタもマルチモーダルの波に乗ろうとしている。メタは5月17日、新モデル「Chameleon」のテクニカルレポートを発表した。\n\nChameleonは、異なるモダリティのモデルを組み合わせるのではなく、ネイティブにマルチモーダルになるように設計されている。上記OpenAIの事例で言えば、以前の3つのモデルを組み合わせるアプローチ（レイト・フージョン＝late fusion）ではなく、入力から出力までの一連の処理を単一のニューラルネットワークで行う方式（アーリー・フージョン＝early fusion）と同様のアプローチということだ。\n\nレポートでは動画入力に対応できるかどうかの言及はなかったものの、研究者らは「Chameleonは、マルチモーダルコンテンツを推論し生成する、統一基盤モデルの実現に向けた重要なステップ」と述べており、今後動画対応できるバージョンが発表される可能性もある。\n\nマルチモーダルAIエージェントの登場により、生成AIのユースケースはさらに拡大する見込みだ。また動画対応と高速化により、スマートグラスなどウェアラブルデバイスとの組み合わせで利用するシーンも増えてくるかもしれない。\n文：細谷元（Livit）,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240908-00010000-ampreview-000-1-view.jpg?pri=l&w=640&h=426&exp=10800'],['https://news.yahoo.co.jp/articles/88c6b17c03d79e27d8d53e78d08396347f246b89/images/000']
