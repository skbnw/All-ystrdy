headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
仕事が“超早い”人は何が違う？ 10倍の目標でも達成できる「優れた上司」になる方法（ビジネス＋IT）,https://news.yahoo.co.jp/articles/9f80792ebceb9549fe8458f210ad86727a3174b8,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240530-00141079-biz_plus-000-1-view.jpg?exp=10800,2024-05-30T07:10:05+09:00,2024-05-30T07:10:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,3913,\n同じ時間で仕事を早くさせるにはどうすれば良いか（Photo/Shutterstock.com）\n10倍の目標を達成する…そんな夢のような話があるのでしょうか？ 答えはYESです。ただ個人ではできません。実現させるには、チームとして「仕組み化」し、10倍の効率化を実現する必要があります。そこで重要になるのが「部下を理解すること」です。そこで今回はエグゼクティブコーチ 名郷根 修（ハイパフォーマンス 代表取締役）の著書『10x 同じ時間で10倍の成果を出す仕組み』より、「部下をしっかりと理解すれば『10倍の仕事効率』が生まれる」という仕事の方程式について解説していきます。\n【詳細な図や写真】10倍の目標を達成できる優れた上司になるには？（Photo/Shutterstock.com）\n10倍の仕事効率を生む「4ステップ」\nまずは、仕事の効率を10倍にする大まかなステップを知る必要があります。大きく次の4つです。\n\n\n・（1）「10倍の目標」を立てる\n\n（2）「好き」「得意」「人の役に立つ」「お金を生む」という4条件に特化する\n\n（3）「どうやるか」以上に「誰とやるか」を重視する\n\n（4）チームを作って「仕組み化」する\n\n それぞれ説明していきます。\n\n（1）「10倍の目標」を立てる\n 「10倍の目標」は、これまでの延長線上で前年比10％成長といった基準で目標を立てるのではなく、10倍の未来から現在を過去のように見る視点を持って目標を立てます。\n\n（2）「好き」「得意」「人の役に立つ」「お金を生む」という4条件に特化する\n これらの4つの条件を満たす能力を「ユニークアビリティ」と呼んでいます。ユニークアビリティとは、人が情熱を持って仕事に取り組める能力です。この能力を生かして仕事をすることで質の高い仕事が可能となり、自分をヒーローであるかのように感じられる能力です。\n\n これは、自分自身が本来持っているユニークアビリティを生かして、自分が望む人生を実現するという考え方です。ここで注意すべきポイントは、ユニークアビリティは「好き」「得意」「人の役に立つ」「お金を生む」の4つの条件をすべて満たしていることです。\n\n（3）「どうやるか」以上に「誰とやるか」を重視する\n （2）のステップで自分のユニークアビリティを自覚しました。もし自分が「好き」ではなく「得意」でない分野がある場合、それを「好き」で「得意」な人は誰なのかを考えます。なぜなら、自分が「好き」ではなく「得意」でないことは、ほかの誰かのほうがより良い結果を出すことができるからです。そしてその時間で同時に、自分自身は「好き」で「得意」なことに集中して取り組みます\n\n\n（4）チームをつくって「仕組み化」する\n （3）のステップで「10倍の目標」を達成するために必要なのは「誰」かを考えた後は、その人たちとチームをつくって「仕組み化」します。\n「好き」「得意」「人の役に立つ」「お金を生む」をどう満たす？\nここからは、仕事の効率を10倍にする4つのステップについてより詳しく解説します。まずは「好き」「得意」「人の役に立つ」「お金を生む」の4条件について、大きく3つの題目にわけて深掘りしていきます。\n\n■「やりたくない」「不得意な」仕事を見直せよ\n もし、「やりたくないこと」や「不得意なこと」の割合が半分以上を占めるとしたら、仕事の内容を見直したほうが良いでしょう。「やりたくないこと」「不得意なこと」を我慢しながら、大きな成果を出し続けることはできません。\n\n まずは、普段あなたが行っている仕事の中で、「やりたくないこと」や「不得意なこと」を減らして、自分が「好き」で「得意」なことを増やしてください。その過程で、あなたが「やりたくないこと」や「不得意なこと」を任せられるユニークアビリティを持ったチームメンバーを探すこと（後ほど解説します）が次のステップになります。\n\n 最初は自分のユニークアビリティを明確にし、次にチームメンバーのユニークアビリティを把握して、それぞれがユニークアビリティを生かせるかたちを目指します。\n■「好き」と「得意」は違う\n 「『好き』と『得意』は同じようなものではないのですか？」と思うかもしれませんが、違います。取り組んでいて喜びや充実感を得られるのが「好き」なこと。「好き」ではなくても、上手にできることが「得意」なことです。\n\n 「好きな仕事だったら、やり続ければ得意な仕事になるんじゃないか？」という考え方も一理ありますが、相応の時間を要します。重要なポイントは、自分が「好き」で「得意」な仕事に注力することが最も生産性が上がるということです。自分が「好き」ではないことや「不得意」なことを減らして、それが「好き」で「得意」なチームメンバーに任せてください。\n\n また自分が「好き」ではない仕事や「不得意」な仕事を、システムやAIに任せて自動化することで、仕事を減らすこともできます。\n\n■ただの「好き」「得意」なら、仕事ではなく趣味\n たとえ自分が「好き」で「得意」な仕事を明確にして、その割合を増やしたとしても、「人の役に立つこと」でなければ人から喜ばれることもありません。そして「お金を生む」ことでなければ売上や利益を生み出せず、ビジネスとして成果を得られません。\n\n 情熱があっても「人の役に立つ」ことや「お金を生む」ことでなければ、仕事ではなく趣味になり、ビジネスにしていくことはできません。だからこそ、ユニークアビリティの4つの条件の残り2つは「人の役に立つ」と「お金を生む」なのです。\n一緒にやる「誰か」を見極める「5つのポイント」\nいったん自分の未来の目標が明確になったら、「どのように達成するか」ではなく、「この目標の実現を助けてくれるのは誰か？」を考えます。これを考える時に、社員について理解を深めておくと良いでしょう。\n\n 「この目標の実現を助けてくれるのは誰か？」は、次のように考えていきます。\n\n（1）人材を選ぶ\n あなたのビジョンや目標に共感し、チーム全体の方向性に貢献できる人材を選びます。\n\n（2）「どの領域で」その人材を生かすか考える\n 適切なポジションに人材を配置することで、成果を最大化できます。それぞれのスキルや専門知識を考慮し、それに応じた役割を与えることが重要です。\n\n（3）「どのくらいの金額」がかかるか検討する\n 最も優れた人材には、その働きに見合った報酬を提供する必要があります。適切な報酬体系を整えることで、人材のモチベーションや忠誠心が高まります。\n\n（4）「どのようなビジネスの影響があるか」を見極める\n その人材がビジネスにどのような価値をもたらし、どのような成果を生み出せるかを考慮します。相手の能力や経験がビジネスの成長や競争力向上に寄与できるかを見極めます。\n\n（5）「ほかにどんな影響が考えられるか」を検討する\n 1人の人材がチームや組織に与える影響は決して少なくありません。その人材がチームメンバーの協力体制にどのような影響を与えるか、チームのシナジーを高めることができるかを考えます。\n\n たとえばユニークアビリティに基づいて仕事を任せる人であれば、必ずしも専門家である必要はありません。仮に、任せる仕事の経験が少ない新入社員でも、その仕事が得意分野で情熱を持って取り組めるのであれば、その新入社員と組織の成長のために任せてもOKと判断できます。その新入社員が新しい発想で結果を出すことができる可能性も秘めています。\n10倍の目標を達成するための「仕組み化」の方法\n続いて行うべきが「仕組み化」です。「10倍の目標」を実現するには4つのステップのうち、これまでお伝えしてきた（1）「10倍の目標」を立てる、（2）「好き」「得意」「人の役に立つ」「お金を生む」という4条件に特化する、（3）「どうやるか」以上に「誰とやるか」を重視する、のプロセスを実践していくのが「仕組み化」への第1段階になります。\n\n そして第2段階は、より具体的なアクションへとつなげていくべく「仕組み」に落とし込みます。\n\n 「10倍の目標」を「仕組み」に落とし込む際には、まず自分が本当に実現したい理想を明確にして、目的を達成するための「10倍の目標」の数字を達成する期間とともに決めます。\n\n 次に、目標を達成するまでのマイルストーンを明確にしていきます。5年目は年商○○億円、3年目は年商△億円、1年目は□億円といったかたちで、大まかで良いので最終目標を達成するまでの目安となる目標を未来から逆算して設定します。こうして1年後に達成する目標も逆算できるようになります。\n\n 1年後の目標を設定したら、定期的に進ちょくを確認します。この際も、メンバーの意識やモチベーションを管理しておくと、役に立ちます。\n\n こうして、「10倍の目標」を達成するための発想を得る質問や「どうやるか」以上に「誰とやるか」という視点も取り入れ、チームによる振り返りと改善も含めて「仕組み」として続けます。このようにして「10倍の仕事効率」は生まれるので、「誰とやるか」を考えたり、「仕組み」として続けたりするのに、部下を理解することは重要なのです。\n執筆：ハイパフォーマンス 代表取締役 名郷根 修,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240530-00141079-biz_plus-000-1-view.jpg?pri=l&w=640&h=365&exp=10800'],"['https://news.yahoo.co.jp/articles/9f80792ebceb9549fe8458f210ad86727a3174b8/images/000', 'https://www.sbbit.jp/article/cont1/141079#image178172']"
ロボットは大規模基盤モデルでどう変わる？まだまだ「賢くなる」、最新研究の数々（ビジネス＋IT）,https://news.yahoo.co.jp/articles/512404ea5c7bd79af4fedd1e9aff1d3269295167,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240530-00139334-biz_plus-000-1-view.jpg?exp=10800,2024-05-30T06:50:05+09:00,2024-05-30T06:50:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,9740,"\n「Dry-AIREC」。JSTムーンショット型研究開発事業：目標3（1人に1台一生寄り添うスマートロボット）では、AIとロボットの共進化で人と共生する汎用（はんよう）ロボットの開発を目指している。これは横浜で5月に開催されたロボティクスとオートメーションの国際会議「IEEE ICRA2024」に出展されたときの様子（写真：筆者撮影）\n大規模基盤モデルを使ってロボットに世界を理解させ、汎用（はんよう）性を持たせようとする試みが盛んになってきた。目指すところは、日常言語による指示の意図を適切に理解し、初めての環境にも対応して作業が行える「汎用ロボット」の実現だ。物体認識の研究発展において大規模画像データベースの「ImageNet」が重要な役割を果たしたように、汎用ロボット実現のためにも学習用データセットを整えることが、まずは重要となる。日本国内はもとより世界各国でさまざまなプロジェクトが進められている。その概要とロボットのこれからの可能性を追ってみたい。\n夢の存在「家事ロボット」が実現？\n1ページ目を1分でまとめた動画\n\n\n\n 読者の皆さまもご存じのとおり、現在のロボットはもっぱら工場や物流倉庫のなかで使われている。決まった環境で、決まった作業をプログラムどおりに実行する。\n\n 一方フィクションの世界では昔から「家事ロボット」が夢の存在として登場している。実際問題、多くの家事が家電で行えるようになった今でも、細かい仕事が作業と作業の間に残り続けている。それらの雑用も含めて自動でこなしてほしいという願いは消えていない。だが家庭のなかでさまざまな作業をさせるためには、無限に近い組み合わせがある状況下で、あいまいな指示に対応して動ける能力が必要とされる。\n\n そもそも論として「お手伝いさん」のような人間サイズのロボットを家庭に迎え入れることが本当に現実的かどうかはさておき、技術として追求するための研究は今でもあちこちで進められている。ロボットと人間のインタラクション、日常を想定したシーンのなかでロボットにタスクをさせる競技大会「ロボカップ@Home」はそのための試みの1つである。\n\n 以前は話者推定などに苦労していたが、昨今はそこに苦労することはなくなった。また状況の認識にもChatGPTなどでおなじみ大規模言語モデル（LLM）が活用されるようになり、競技で競うべき内容も変わりつつあるようだ。ただ、大規模言語モデルがあれば何でもできるという話でもなく、汎用サービスロボットへの道のりはまだ遠い。ロボカップでも、基本となる物体データや、異なるロボットに転移学習させることができる共通スキルのモデルをそろえて共有しようといった動きもあると聞く。\n\n\n\nRoboCup 2024 @Homeの予選ビデオ\n\n\n スタンフォード大学 Vision and Learning Labなどによる「BEHAVIOR-1K」では、洗濯や片付け、テーブルセッティングなど、1000の日常的な家事活動を「身体化人工知能（Embodied Artificial Intelligence、EAI）」でこなすことを目指すプロジェクトが進行している。「OMNIGIBSON」というリアルかつ物理的なシミュレーション環境を活用する。\n\n 「BEHAVIOR-1K」は2つのコンポーネントからなる。1つ目は8つのシーンタイプ、家やオフィス、レストランや庭など50のインタラクティブシーン、注釈付きの1900以上のオブジェクト・タイプ、9000以上のオブジェクト・モデルを含む、日常活動の定義からなるデータセットだ。\n\n 2つ目が「OMNIGIBSON」環境である。これはデジタルツイン開発などに用いられているNVIDIAのメタバースプラットフォーム「Omniverse」をベースとして開発されたシミュレーション環境で、柔軟なマテリアルと変形可能なボディ、リアルな流体と熱エフェクトなどもサポートする。\n\n この環境を使って、人間本位、多様性、リアリズムを元にしたロボット学習ソリューションの研究開発を目指すという。なお「BEHAVIOR」は「Benchmark for Everyday Household Activities in Virtual, Interactive, and EcOlogical EnviRonments」の略だ。\n\n 似たような考え方のプロジェクトは世界各国で行われている。今回はそのような試みをいくつか紹介しておきたい。どれでも良いので成功してもらいたい。\n\n なお、本連載では2023年4月にも「大規模言語モデルでロボットはどう進化するのか、いい意味で「予測不可能」な未来とは」でこの話題を取り上げている。今回はその続きだと思ってもらいたい。ただ、この分野の発展は本当に早いので、あくまでスナップショットの1枚だと思ってもらったほうがいいかもしれない。\nGoogle DeepMindのロボット工学モデル 「RT-X」\n話をいったん基本的なところに戻す。現在「大規模基盤モデル」の活用が注目されている。タスクごとに個別に詳細なプログラミングを行ってロボットを動作させるのではなく、乱雑な日常空間にあるようなさまざまな作業、いわゆる汎用作業に適応させる手法としての活用だ。\n\n\n\nRT-Xのアニメーション\n\n\n 「大規模基盤モデル」とは多種多様なデータで学習させた大規模ニューラルネットワークである。基盤モデルを元に「ファインチューニング」することで、さまざまなタスクに適応させることができる。\n\n 計算量とデータ量とパラメータ数を大規模にすることで、大規模言語モデルは飛躍的に性能を伸ばし、多様なタスクに対応できるようになった。仕組みとしては確率モデルなのだが、実際に実現できていることは驚きとしか言いようがない。\n\n しかも猛烈な速度で進化し続けている。OpenAIが2024年5月13日に発表した最新モデル「GPT-4o」との会話はまるで人間のようだ。リアルタイムで逐次通訳までやってくれる。学習データの偏りによるのか、日本語の発話には外国人訛（なま）りがあり、性能も英語よりも劣るようだが、OpenAIは日本にも拠点を設けているので、今後の発展に期待したい。\n\n\n\nGPT-4oのイントロダクション\n\n\n GPT-4oの話はいったん横に置いておき、ロボットの話に戻ろう。同様に、大規模な確率モデルを使うことでロボットに周囲の環境や求められているタスク目標を認識・理解させたり、動作させるためのコードを自動生成させて、タスクを実行させたりしようという考え方がある。たとえばロボットの知覚処理に大規模モデルを使うことは容易に想像できる。\n\n Google DeepMindは各国の数十の大学と共同で「RT-X」というプロジェクトを提案している。「RT」はRobotics Transformerの略で、Transformerアーキテクチャーをベースとしていることを意味する。RT-Xは2つのTransformetモデルから構成される。リアルワールドのロボットから学習させたRT-1-Xと、Webとロボットのデータから自然言語に応答できるようにしたRT-2-Xというモデルだ。2つのモデルを組み合わせることで高い汎化（はんか）性能を持たせられるという。\n\n DeepMindは異なるロボットから収集した多様な動作や視覚データなどからなる「Open X-Embodiment」という巨大データセットを作り、あいまいな自然言語にしたがっていろいろなタスクに適用できる動作モデルを作ろうとしている。詳細はこちらのブログで解説されているが、100万以上のエピソード、500以上のスキルと15万以上のタスクを実証する22種類のロボットからデータを収集した包括的なデータセットだという。\n\n このデータセットを使ってRT-1-Xを訓練し、ものをつかんだり移動させたりさせる実験を行ったところ、従来のモデルよりも50％ほど成功率が高くなったとされている。RT-2-Xのほうは別のロボットの別のデータセットを使って訓練したら3倍の能力を発揮した。また、元のモデルにはなかった空間理解、指示の理解の能力を見せたという。 たとえば「move apple on cloth（リンゴを布の上に）」と「move apple near cloth（リンゴを布の近くに）」という指示では、ロボットが取るべきアームの軌道は異なるが、それに対応できたというのだ。\n\n つまり、ほかのロボットのデータを組み合わせると、ロボットがより賢くなる可能性があるというわけだ。「多くのデータを集めれば集めるほどロボットを賢くすることができるのではないか？」という考え方は以前からあったのだが、それが本当になりつつあるのかもしれない。\n\n なお、5月に横浜で行われたロボティクスのトップカンファレンス「ICRA2024」で「RT-X」はベストペーパー賞を獲得した。ただし、実際に試すとそれほどうまくは動かないとも聞く。\nNVIDIAのヒューマノイドプロジェクト「GR00T」\nGPUで知られるNVIDIAは2024年3月にヒューマノイド ロボットを対象にプロジェクト「GR00T」を発表した。CEOのジェンスン・フアン自身が、1X Technologies、Agility Robotics、Apptronik、Boston Dynamics、Figure AI、Fourier Intelligence、Sanctuary AI、Unitree Robotics、XPENG Robotics など多くのヒューマノイドのCGを背景に派手なパフォーマンスを行って発表した様子はとても印象的だったので記憶にも新しいだろう。Agility Roboticsの「Digit」は実物も登場してデモを行った。Digitは安定感ではほかよりも2歩くらい先を行っているようだ。\n\n\n\nGTC 2024 年 3 月基調講演、NVIDIA CEO ジェンスン ファン氏\n\n\n 「GR00T」は「Generalist Robot 00 Technology」の略とされている。やはり自然言語を理解して、人のように行動できる器用なロボットを実現することを目指す。NVIDIAのデジタルツインの「Omniverse」、Transformerアーキテクチャーの処理に最適されたGPUのほか、ツールとしてはIsaac Robotics プラットフォームを用いる。Isaacには強化学習やクラウド学習用マネジメントサービスなどが含まれており、トレーニング済みモデルや各種ライブラリなども提供される。「GR00T」自体はヒューマノイド用とされているが、その他のフォームファクターのロボットによる移動やハンドリングなど一般的な作業においても新たなレベルの自律性を持たせることを目指している。\n\n NVIDIAのような企業による後押しが得られることは多くのロボットスタートアップにとって、とても心強いことだろう。計算リソースの心配がなくなることで、大きな成果が得られることを期待したい。\nAIエージェント「Eureka」\nNVIDIA Researchは「Eureka」というAIエージェントを開発している。OpenAIの大規模言語モデルの「GPT-4」を使ったもので、ロボットをトレーニングするための報酬関数の生成を改善するようGPT-4に指示をして、NVIDIA Isaac Gym で強化学習させることができる。\n\n\n\nユーレカ LLM による究極のロボットの器用さ\n\n\n シミュレーションでトレーニングデータを蓄積して方策を学習させ、それを実機・実環境に持っていく「Sim2Real」という考え方があるが、この「Eureka」を活用し、現実とシミュレーションのギャップを自動で埋めるために大規模言語モデルを使おうというプロジェクトが「DrEureka」で、NVIDIAやペンシルバニア大学らが進めている。\n\n Sim2Realのアプローチは、タスクの報酬関数とシミュレーションにおける物理パラメータの手動での設計とチューニングに依存している。それを大規模言語モデルを使って自動化・高速化する。「Dr」はドメインランダム化を意味しており、Eurekaで報酬関数とポリシーを自動生成したあと、再度、異なるシミュレーション条件下でポリシーをテストするために、物理パラメータをランダム化したセットをLLMで生成する。その条件でトレーニングを行うと、より適応力が増したという。\n\n このアプローチを使うことで、4脚ロボットにバランスボールを歩かせるといったタスクを繰り返し手動設計することなく実現できたという。\n\n\n\nDrEureka の5分間のノーカット導入ビデオ\n物流分野での大規模基盤モデル活用「Covariant」\n大規模基盤モデルの適用対象はもちろんヒューマノイドだけではない。本丸は、むしろすでに普及している産業用ロボットだろう。産業用ロボットがより柔軟なタスクをこなせるようになると、それだけで大いに意味がある。特に、さまざまな未知の物体を扱わなければならないため、最初から汎用性がある程度必要とされる物流現場での期待は大きい。\n\n\n\n把握の改善に関するコンテキスト内学習\n\n\n 機械学習のロボットへの適用の第一人者として知られるカリフォルニア大学バークレー校のピーター・アビール（Pieter Abbeel ）教授らが2017年に創業したスタートアップのCovariantはピッキング用AI「Covariant Brain」を物流向けに展開している。同社は以前から階層型強化学習（Hierarchical Reinforcement Learning）によるロバストな知覚や汎化性能獲得を目指しており、「Covariant Brain」を実際の現場で稼働させてデータを収集してきた。\n\n 特に産業用ロボット大手の1つABBとは親密な関係を構築している（2020年2月のリリース）。\n\n\n\nABBとCovariantは堅牢なAIソリューションを開発するために提携\n\n\n リアルワールドでのマルチモーダルなロボットデータを大量に使って学習させた大規模モデルである「RFM-1」は、テキスト、画像、ビデオ、ロボットの動作、およびさまざまな数値センサーの読み取り値に対して学習された80億パラメータの変換器である。同社の解説によれば、「RFM-1」を使うことでロボットは自然言語を使った指示の元でいろいろな物体の把持動作や動作シーケンスを生成したり、それをこなしたあとの状態の動画像を生成したり、その途中のセンサー値をシミュレート、つまり予想することができる。さらに学習させることでより多くのタスクに対応できるという。\n中国スタートアップ「X Square」\n中国でも「X Square」というスタートアップによるロボット向け汎用大規模モデルのプロジェクトが進められているようだ。2023年に創業されたばかりの新興企業だが、「36kr」の記事によれば、認識から動作までのEnd-to-End学習の能力を持たせようとしており、わずか3カ月でロボットに数十種類の複雑なタスクを実行させられるようになったという。\n日立と早大の「深層学習型ロボット制御技術」\nEnd-to-End学習とは、1つのタスクをサブタスクに分解し、複数のモデルを組み合わせてほどくのではなく、1つのモデルでまとめてほどいてしまおうというものだ。ディープラーニングが成果を出し始めた近年、特に盛んになった。\n\n 日立製作所と早稲田大学 尾形 哲也研究室では、ロボットの手先や対象物の特徴などを「2人羽織」のようなかたちでロボットを人が動かして「模倣学習」させることで、触ると変形するような柔軟な物体、つまり布やケーブルなどをプログラミングなしで扱うことができるシステムを研究している。\n\n\n\n形状が変わる物体のハンドリングに対応する深層学習型ロボット制御技術\n\n\n リリースでは「対象物の物理的特徴（色や形状など）と動作内容（つかむや押すなど）を含んだ言語指示から、学習していない動作を連想して実行することが可能」とされている。\n\n ロボットにやらせたい動作を複数回教えるだけで、ロボットの視覚・触覚情報から着目すべき情報（対象物の位置や方向など）と、そのときに取るべき動作を自動抽出することで、不定形物を扱うような作業に対する動作も、プログラミングレスで実行することが可能だ。ロボットが導入されていない現場作業を自動化することで、人の作業を支援するシステムを構築することを目指す。\n\n\n\n現場の状況とモデルの予測誤差を最小化する深層予測学習のロボット制御技術を用いて、自律的にドアを開け通過する機能を実証\n\n\n ここで日立が使った遠隔操作装置は最近ニーズが増している。特に有名なものがスタンフォード大の研究者らが進める「Mobile ALOHA」だ。比較的、安価な遠隔操作用ハードウェアを使って（とはいっても、少なくもロボットアームは4つ、カメラも複数付いているので、それなりの値段だ）、モバイルマニピュレータ型のロボットに日常的なタスクを学習させるためのデータ収集を行おうという考えだ。スタンフォードの研究者たちは、タスクごとに50回程度試行したら、9割程度の成功率を達成したとしている。\n\n\n\nMobile ALOHA スマートホームロボット 自律スキルの集大成\n\n\n 「Mobile ALOHA」はオープンなプロジェクトなので、同様の全身遠隔操作用の機材は、AgileX Robotics社の「COBOT MAGIC」など、あちこちから出てこようとしている。ニーズがあるということだろう。日立の機器も、あくまで研究用途として販売すると聞いている。\n\n\n\nMobile ALOHAとCobot Magicでネジ締めが簡単に\n「世界モデル」と大規模基盤モデルの融合\nもう1つ、大事な概念が「世界モデル」である。ロボットやAIを動かす上で、物理世界の物体がどのように変化するのか、どんな構造をしているのかを学習によって獲得し、予想できるようにするための、「世界」のモデルである。\n\n たとえば、物体の裏側がどうなっているのか、人間や動物なら暗黙の前提として知っている（もしくは推定できる）ような知識を表現したモデルだ。一言でいえば「常識」である。その常識を反映させた「世界モデル」を使うことで、経験のない環境においても物体操作したときに、どんな変化が起こるのかを常識的な推定を行いながらロボットは動けるようになる。\n\n ロボットがセンサーで知り得ることにはおのずから限界がある。たとえば、奥行きセンサーを持っていなければ、奥行きの情報は取れないし、硬さを測るセンサーがなければ硬さは分からない。だがほかのセンサー情報を使えば、得られない情報を推定することも、ある程度はできるはずだ。人間も同じようなことをやっている。\n\n またもちろん、大規模なシミュレーションや教示による学習データをそろえることができないときにも、世界モデルを組み合わせて活用することは問題解決に有効な方法となるはずだ。\n\n Google DeepMindは「Genie：Generative Interactive Environments」という、テキストや画像からインタラクティブな仮想環境を生成できるモデルを発表している。ラベル付けされていないインターネット動画から教師なしで学習を行った生成的インタラクティブ環境で、プレーヤーは生成された世界のなかで自由に動くことができる。110億パラメータを持つ「基盤世界モデル」だ。\n\n これがロボットにも使えるのである。アームを使って物体操作をすると物体が変形する。その変形などを理解できるようになる。単に「以前の状態がこうだから次はこう遷移する確率が高い」といった予測だけではなく、物理的にあり得そうなかたちで世界の変化を予測できるようになるからだ。\n\n また、ネット上のビデオを学習データとして使うための基盤としても使えるという。ゲームエンジンとロボティクスの関係については今後も注目しておきたい。\n\n 今後は、世界モデルと大規模基盤モデルとを組み合わせて使うことで、ロボットが、世界のなかで、より自由に動けるようになることが期待されている。日本企業ではNECが世界モデルに注力している（2024年2月のリリース）。具体的には倉庫での作業内容やレイアウト変化に柔軟に対応できるようになるとされている（2023年3月のリリース）。\n\n\n\n「気が利く」ロボが倉庫の作業に一役 世界が注目するNECの制御AIが届ける社会価値\nロボットがまだまだ「賢くなれる」ことは確実\nここまで特に紹介してこなかったが、もちろん、「大規模言語モデルを使ってロボットに指示を与え、ちょっとしたコードを生成させて簡単な作業をさせる」といった分かりやすい連携デモは各社が行っている。また、ここまで述べてきたような考え方をさっそく実世界で動くロボットに適用して、本当の意味での「理解力」を持たせようとする研究もある。なかには、ヒューマノイドを使ったものもある。\n\n\n\nステータスの更新\n\n\n ここからどのくらいロボットの能力が伸びるのかは未知数だ。ただ、はっきりしていることが少なくとも2つある。1つ目は、ロボットはまだまだパフォーマンスを発揮しきれていない、つまり、まだまだ「性能の伸び代」があるということだ。ロボットはハードウェアなので、もちろん、どんなに頑張ってもハードウェア本来の性能以上のことはできない。だが、それすらまだ性能を引き出せていないのである。より良いAIモデルが開発され、ロボットに接続されれば、ロボットの性能はそのぶん伸びる。\n\n もう1つはロボットの行動能力を伸ばし、もっと柔軟にするための有効らしき方策として、さまざまな形態や利用シーンのデータを大量に学習させたモデルが有効である可能性が高い、ということだ。これが最近の成果である。\n\n いろいろなデータセットが作られていることは前述のとおりだが、それらをひとまとめにして扱えるようにすると、もっともっとAIの性能は改善され、ロボットは高い汎化性能を持つ可能性が高い。\n\n マイクロソフトがOpenAIに10億ドルの投資を行ったのは2019年であることからも分かるように、「大規模言語モデル」の研究開発自体は以前から進められていた。そして2022年の「ChatGPT」発表で、いきなりポピュラーになった。同様のことがもしロボットで起こったら──？起こらないとは限らないし、むしろ起こってほしいと思っている。\n\n ただ、これまではプログラムされたとおりにしか動けなかったロボットが、より柔軟に動けるようになり始めたら、これまでとは異なる種類の問題も起きてくるだろう。ロボットが何を考え、何をしようとしているのか、人間とコミュニケーションする必要が出てくることは間違いない。それはまた別の話である。\n執筆：サイエンスライター 森山 和道",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240530-00139334-biz_plus-000-1-view.jpg?pri=l&w=640&h=425&exp=10800'],['https://news.yahoo.co.jp/articles/512404ea5c7bd79af4fedd1e9aff1d3269295167/images/000']
