headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
GIGABYTE、OLEDゲーミングモニターの保証期間を3年に強化（PC Watch）,https://news.yahoo.co.jp/articles/796d7b365466e189fd9927e2a432164e75416897,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000157-imppcw-000-1-view.jpg?exp=10800,2024-03-25T19:11:53+09:00,2024-03-25T19:11:53+09:00,PC Watch,imppcw,PC Watch,313,"\n写真：PC Watch\nGIGABYTEは、GIGABYTEおよびAORUSブランドで販売するOLEDゲーミングモニターの保証期間を従来の1年間から3年間に更新したことを発表した。\n\n 対象製品は、販売中のAORUS CO49DQや、今後発売されるAORUS FO32U2P、AORUS FO32U2、AORUS FO27Q3、MO34WQC、MO34WQC2など。本発表の前に購入されたAORUS CO49DQについても3年間の保証が付与される。\n\n 同社は、輸送中に発生する画面上の擦り傷などに対処するため、画面表面に保護フィルムを貼り、物理的な保護を強化したことも発表した。\nPC Watch,浅井 淳志",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000157-imppcw-000-1-view.jpg?exp=10800'],['https://news.yahoo.co.jp/articles/796d7b365466e189fd9927e2a432164e75416897/images/000']
本格化する企業での生成AI活用、国産LLMに期待集まる。MM総研調べ（PC Watch）,https://news.yahoo.co.jp/articles/b70d49a94d12ca43c9c73d78aad57ad86f6fec37,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000155-imppcw-000-1-view.jpg?exp=10800,2024-03-25T18:51:26+09:00,2024-03-25T18:51:26+09:00,PC Watch,imppcw,PC Watch,866,"\n写真：PC Watch\n株式会社MM総研は、言語系の生成AIや大規模言語モデル(LLM)の利用動向に関するWebアンケートを企業1,599社を対象に行なった。これによれば、2024年度から企業の生成AI活用が本格化する中で、72%の企業が国産LLMに期待を寄せているという。\n【画像】生成AI利用の現状と今後の方針(出典:MM総研)\n調査時点での、企業における生成AIの導入率は19%で、本格導入しているのは6%にとどまった。一方で、導入準備中や検討中の企業は24%存在し、2024年度には導入が加速する見込みだ。\n\n 導入企業における生成AIの利用方法は、Webブラウザ経由で生成AIサービスを利用するのが最も多く60%で、次いで生成AIやクラウドのベンダーが提供するAPIを利用するパターンが44%であった。\n\n まずはChatGPTやGeminiなどを手軽に利用した企業が多いとみられている。また多くの企業が、今後2年で生成AIの活用を少人数による試験環境から全社導入へ推進する考えであるという。\n\n 生成AI導入にあたって課題を感じている企業は97%に上り、特に「AIやデジタルの高度な知識および技術を持つ人材が足りない」と「セキュリティやプライバシーの確保」が最も多くの企業に共通する課題となっている。\n\n 国産LLMに期待している企業は72%と多く、特にNTTの「tsuzumi」への期待が最も高い。国産LLMに期待する理由としては「日本語や専門用語への対応」、「費用を安く済ませる」、「サポートの充実」、「データを外部に出すことなく利用できること」が挙げられている。\n\n 生成AIの導入や利用拡大で期待されているベンダーは、1位から順にNTTデータ(17%)、富士通(16%)、日本IBM(10%)、NEC(7%)、アクセンチュア(7%)となっている。これらのベンダーは、生成AIに関する製品やサービスの提供が早いことや、情報発信量の多さが評価されている。\nPC Watch,浅井 淳志",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000155-imppcw-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/b70d49a94d12ca43c9c73d78aad57ad86f6fec37/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1578/982/html/02_o.jpg.html']"
Fractal Design、450Wの小型電源と同電源を搭載したスリムなPCケース（PC Watch）,https://news.yahoo.co.jp/articles/bef9f94e40a0c1e45f71171349b344cb760d3f09,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000108-imppcw-000-1-view.jpg?exp=10800,2024-03-25T14:58:55+09:00,2024-03-25T14:58:55+09:00,PC Watch,imppcw,PC Watch,802,"\n写真：PC Watch\n株式会社アスクは、Fractal Design製のSFX電源「Anode SFX Bronze 450W」および同電源ユニットを標準搭載したスリム型PCケース「Node 202 Black w/ Anode SFX Bronze 450W」を29日に発売する。実売予想価格は順に、1万1,880円前後、2万6,400円前後の見込み。\n【この記事に関する別の画像を見る】\nAnode SFX Bronze 450Wは、80PLUS BRONZE認証を取得したSFX電源ユニット。静音性に優れた80mmファンを内蔵するほか、UltraFlexケーブルにより、ケーブル直付けタイプながら限られた内部スペースでの配線も容易とする。\n\n 主な仕様は、合計最大出力は450Wで、各ライン出力は+3.3Vが16A、+5Vが16A、+12Vが37.5A、-12Vが0.3A、+5Vsbが2.5A。コネクタはマザーボード用24ピン、CPU用4+4ピン、PCIe用6+2ピン×2、SATA用15ピン×3。本体サイズは125×100×63.5mm。\n\n Node 202 Black w/ Anode SFX Bronze 450Wは、スリムなMini-ITX対応ケース「Node 202 Black」に、Anode SFX Bronze 450Wを搭載したセットモデル。横置きと縦置きが可能で、最大310mmまでのビデオカードを搭載できる。拡張スロット数は2基で、全高56mmまでのCPUクーラーを装備できる。\n\n 搭載ベイ数は2.5インチシャドウベイ×2。搭載可能ファンは120mmファン×2(ビデオカード非搭載時のみ)。前面インターフェイスは、USB 3.0×2、音声入出力。横置き時の本体サイズは377×332×88mm、重量は3.5kg。\nPC Watch,浅井 淳志",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000108-imppcw-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/bef9f94e40a0c1e45f71171349b344cb760d3f09/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1578/855/html/02_o.jpg.html']"
パソコン工房、Core Ultra 5/7搭載の14型モバイルノート（PC Watch）,https://news.yahoo.co.jp/articles/8bd8216fd37370cdc352d5a505b18ff3a5b76d43,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000151-imppcw-000-1-view.jpg?exp=10800,2024-03-25T18:16:26+09:00,2024-03-25T18:16:26+09:00,PC Watch,imppcw,PC Watch,482,"\n写真：PC Watch\nパソコン工房を運営する株式会社ユニットコムは、IntelのCore Ultraプロセッサ搭載の14型モバイルノートを2モデル発売した。価格は15万4,800円から。\n\n マグネシウム合金採用で重量0.95kgと軽量設計が特徴のモバイルノート。液晶はWUXGA(1,920×1,200ドット)解像度の14型非光沢液晶を搭載する。\n\n STYLE-14FH128-U5-UH2Xの主なスペックは、CPUがCore Ultra 5 125U、メモリがDDR5 8GB(オンボード)、ストレージが500GB M.2 NVMe SSD、OSがWindows 11 Homeなど。\n\n STYLE-14FH128-U7-UH2Xの主なスペックは、CPUがCore Ultra 7 155U、メモリがDDR5 8GB(オンボード)、ストレージが500GB M.2 NVMe SSD、OSがWindows 11 Homeなど。\n\n 本体サイズは約312.5×223×23.3mm、重量は約0.95kg。\nPC Watch,鈴木 悠斗",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000151-imppcw-000-1-view.jpg?exp=10800'],['https://news.yahoo.co.jp/articles/8bd8216fd37370cdc352d5a505b18ff3a5b76d43/images/000']
フォントワークス、中川翔子さん書き下ろし「しょこたんフォント」（PC Watch）,https://news.yahoo.co.jp/articles/a8eb7ab0c153e0a5eb5ec0c9e5785796387d2e5e,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000122-imppcw-000-1-view.jpg?exp=10800,2024-03-25T15:57:46+09:00,2024-03-25T15:57:46+09:00,PC Watch,imppcw,PC Watch,362,"\n写真：PC Watch\nフォントワークス株式会社は、タレントの中川翔子さんが約8,000字を書きおろした手書き書体「中川翔子のしょこたんフォント」を発売した。手書き文字と99種類の手書き絵文字のセットで価格は2,990円。\n【画像】手書き書体\n「ころんと可愛く優しい曲線と勢いが特徴の、親しみやすく元気で明るい印象の書体」といい、ファンのみならずナチュラルな手書き書体を求める人にも向くとしている。ウエイトはふつう、やや太め、太めの3展開。また、絵文字も書き下ろしで、、中川翔子さんならではの猫やギザ、マミタスといった変わり種絵文字も収録している。\n\n また、フォントデータに吹き出しの文字が変えられるオリジナルアクリルスタンドがついた特装版も同時発売する。価格は6,290円。\nPC Watch,鈴木 悠斗",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000122-imppcw-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/a8eb7ab0c153e0a5eb5ec0c9e5785796387d2e5e/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1578/874/html/02_o.jpg.html']"
Windowsの「メモ帳」にスペルチェック機能を実装（PC Watch）,https://news.yahoo.co.jp/articles/0967e1298dfae3dd9dd8f406350770c18597dcd9,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000083-imppcw-000-1-view.jpg?exp=10800,2024-03-25T13:05:20+09:00,2024-03-25T13:05:20+09:00,PC Watch,imppcw,PC Watch,310,"\n写真：PC Watch\n米Microsoftは21日(米国時間)、Windows Insiders向けの更新として「メモ帳」にスペルチェックとオートコレクトの機能を実装したと発表した。更新後のバージョンは「11.2402.18.0」となる。\n\n Windows 11のCanary/Devチャネル向けで、スペルミスの単語を強調表示し、修正候補も表示するもの。スペルチェック機能は複数言語をサポートしている。\n\n 同機能はログファイルやコーディングなどの一部ファイルタイプでは初期設定で無効化されている。それらのファイルタイプでは設定から有効化することで利用できるとしている。\nPC Watch,鈴木 悠斗",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000083-imppcw-000-1-view.jpg?exp=10800'],['https://news.yahoo.co.jp/articles/0967e1298dfae3dd9dd8f406350770c18597dcd9/images/000']
TSUKUMO、Crucial製メモリ/SSD採用でGeForce RTX 4070 SUPER搭載のゲーミングPC（PC Watch）,https://news.yahoo.co.jp/articles/7d1ffe477796b5fadba64a89fd8b34e80dbd8771,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000057-imppcw-000-1-view.jpg?exp=10800,2024-03-25T11:28:21+09:00,2024-03-25T11:28:21+09:00,PC Watch,imppcw,PC Watch,502,"\n写真：PC Watch\n株式会社ヤマダデンキはTSUKUMOブランドより、Crucial製のメモリとSSDを搭載したゲーミングPC「G-GEAR Powered by Crucial」シリーズから、新モデル「GC7J-H241BN/R/CP1」を発売した。価格は28万3,800円。\n【画像】Crucial製のメモリとSSDを搭載\nGC7J-H241BN/R/CP1は、マイクロン製DRAMやアルミニウム製ヒートシンク採用のDDR5メモリ「Crucial Pro」と、最大転送速度6,600MB/sのPCIe 4.0対応M.2 NVMe SSD「Crucial P5 Plus」を搭載したゲーミングPC。CPUファンはサイドフロー式の空冷タイプで、ステルスブラックの筐体にアドレサブルRGB LEDファンを装備している。\n\n 主な仕様は、Core i7-14700KF、48GB DDR5メモリ、1TB M.2 NVMe SSD、GeForce RTX 4070 SUPER、850W 80PLUS Gold電源、Windows 11 Homeなどを備える。\nPC Watch,宇都宮 充",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000057-imppcw-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/7d1ffe477796b5fadba64a89fd8b34e80dbd8771/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1578/734/html/02_o.jpg.html']"
ノートPCに最大3つ画面を増設できる「Ariescreen 13s」（PC Watch）,https://news.yahoo.co.jp/articles/0f80c98bc17d3d5ba114c0b2bdcb8a6bea0bc87c,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000061-imppcw-000-1-view.jpg?exp=10800,2024-03-25T11:42:28+09:00,2024-03-25T11:42:28+09:00,PC Watch,imppcw,PC Watch,595,"\n写真：PC Watch\n株式会社ギャザテックは、ノートPCに最大3つの画面を増設できる「Ariescreen 13s」について、クラウドファンディングサイトのGREEN FUNDINGにて出資を募っている。製品入手に必要な最低出資額は10万2,960円から。\n【画像】クリップ部\nAriescreen 13sは、ノートPCに装着するクリップ部と、3つの13.3型1,920×1,200ドットのディスプレイモジュールからなる製品。必要に応じてクリップにディスプレイモジュールを装着することで、最大3つの画面を増設することができる。なお、クリップ部にはハブ機能も備えており、ディスプレイモジュールとのケーブル接続も必要となる。\n\n クリップは13～17型のノートPCに対応し、スタンドも装備。ディスプレイモジュールは横と縦のどちらでも装着が可能で、最大140度の回転もできる。\n\n ディスプレイモジュールの仕様は、輝度が250cd/平方m、コントラスト比が1,000:1、色域カバー率がNTSC 45%。インターフェイスは、USB 3.0×4、USB Type-Cによる映像入力および(給電用)。\n\n 本体サイズおよび重量は、クリップ部が29×2.8×14cm/580g、ディスプレイモジュール部が30.6×1.3×20.7cm/593g。\nPC Watch,宇都宮 充",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000061-imppcw-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/0f80c98bc17d3d5ba114c0b2bdcb8a6bea0bc87c/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1578/749/html/04_o.jpg.html']"
WQHDで240Hz VRR対応モニターを買って配信環境を更新しようとしたら、かなりの沼だった。パススルーのまやかしにも遭遇（PC Watch）,https://news.yahoo.co.jp/articles/3db87997f1cca7bb000facb8d7e6c7bddb25c88d,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000015-imppcw-000-2-view.jpg?exp=10800,2024-03-25T06:18:20+09:00,2024-03-25T15:21:28+09:00,PC Watch,imppcw,PC Watch,7830,"\n写真：PC Watch\nそれまで使っていたゲーミングモニターが壊れた。画面の左下あたりにうねった黒いシミがうっすら現われたのだ。白一色のような画面以外ではほとんど気にならないので、もしかしたら何かの弾みに直るかもなどと思いつつ、1カ月ほどはだましだまし使っていたのだが、ちょっとずつシミが濃くなってきた感もあるので新しいものに買い換えることにした。\n【画像】壊れたモニター。写真では目立たないし、ゲーム画面などでも気にならないが、妙なシミが表示されている\n壊れたのは、最大240Hz表示に対応するフルHD(1,920×1,080ドット)のゲーミングモニター。プレイする多くのゲームが、格闘ゲームやFPS系ゲームなど、画質よりも表示速度を重視するタイプなのでフルHD解像度でも十分なのだが、最近ではその1つ上のWQHD(2,560×1,440ドット)でも240Hz対応製品が増えている。また、主なプレイタイトルがWQHDでも快適に動作するのと、DisplayPortだけでなく、HDMIでも240HzかつVRRにも対応するものがちらほら出てきた。\n\n と言うことで時間をかけて吟味し、白羽の矢を立てたのがBenQの「MOBIUZ EX270QM」だ。Amazonでの購入価格は約9万9,000円だった。しかし、結論から言うと筆者が目論んでいた環境は実現できなかった。どのあたりに問題があったのか、説明していく。\n\n■ 裏技(?)でHDMIでキャプチャしつつ、240Hz VRR環境でゲームをプレイ\n\n 筆者は時々個人でもゲーム配信を行なっている。ゲーミングPCと配信PCを別々にして、ゲーミングPCの映像をUSBキャプチャ経由で配信PCに取り込んで配信に載せるという形を取っている。2台配信は、コストがかかり、配線や技術の面でも面倒なのだが、OBSのプラグインを使ってゲーム画面を3次元に傾けたり、その上にリアルタイムで自分を合成したりといった凝ったことをやった結果、ゲームよりOBSの方がGPU負荷が高くなると言う逆転現象が起きてしまったので、やむを得ず2台に分けたのだ。\n\nこんにちは、無駄に配信画面凝り過ぎおじさんです。\n\n今まで、ゲーム画面を出す時、カメラはパンさせて、ゲーム画面は表示/非表示を切り替えてたのを、ゲーム画面もアニメーションするようにしてみた pic.twitter.com/aDlsFhD1xk\n\n― 若杉｜PC Watch編集長/インプレスeスポーツ部部長 (@pcw_wakasugi) October 26, 2023\n\n この2台に分けた時に悩んだのが、キャプチャのやり方だ。元々使っていたモニターはDisplayPortもHDMIも最大240Hzに対応していた。しかし、このHDMIはG-SYNCやFreeSyncといったVRR(Variable Refresh Rage:可変リフレッシュレート)に対応していない。一方で、基本的に一般向けのキャプチャユニットはHDMIにしか対応していない。\n\n つまり、2台構成で配信する場合は、V-Syncをオンにして""テアリングは発生しないが、かくつきが発生する""か、V-Syncをオフにして""かくつきは発生しないが、テアリングが発生する""のいずれかに甘んじるしかない。\n\n だが、どちらもイヤだった筆者は、とある裏技(?)にたどり着いた。それは、同じゲーミングPCから同じモニターにHDMIとDisplayPortの両方を同時につなぐという方法だ。その上で、この2つの出力をWindows上で「複製」状態にする。すると、HDMIでキャプチャしながら、DisplayPort側はG-SYNCを有効にできるのだ。\n\n この場合も、2系統に同時出力する無駄はあるものの、おおきな不満なく使っていた。\n\n■ HDMI 2.1でHDMIでもWQHD 240Hz VRRに対応可能……に見えたMOBIUZ EX270QM\n\n そして、モニターを買い換える事になったわけだが、壊れたから買い換えるという理由でも、スペックは同じではなく、今までより上のものにしようと考えた。その結論が、解像度をWQHDに上げることと、HDMI 2.1対応のものにするということだ。\n\n WQHDにする理由は前述の通りで、HDMI 2.1にするのは、HDMIでも240Hz VRRが実現できるからだ。HDMIは2.0でも帯域的にはWQHDの240Hz表示に対応できる(ただし、YCbCr 4:2:0にはなる)が、VRRには対応しない。\n\n また、ちょうどAVerMediaやElgatoからWQHD 240Hz VRR、つまりHDMI 2.1に対応するキャプチャユニットが登場しており、いずれも検証用にメーカーから提供を受けていた。EX270QMとこれらのキャプチャユニットの組み合わせにより、HDMIとDisplayPortに同時配線というまどろっこしいことをしなくて済むようになる。そう考え、10万円払ってこのモニターを買ったのだ。ちなみに、HDMI 2.1に対応するモニターは今ではそこそこあるが、その大半は4Kで、WQHD製品は希少だ。\n\n しかし、1つ大きな盲点があった。AVerMedia製品、Elgato製品、どちらもこのモニターではWQHD 240Hzのパススルーには""完全対応""できないことが分かったのだ。WQHDでパススルーできるのは144Hzまでとなる。原因はモニターのDSC機能にあった。\n\n DSCとはDisplay Stream Compressionの略でHDMI 2.1から取り入れられた。DSCにより、人間の目では認識できなレベルの映像圧縮を行なうことで、HDMI 2.1では8K 60Hzなどにも対応できるようになっている。DSCは、基本的には8Kなどの高解像度をにらんだ技術なのだが、実はEX270QMのHDMIは、WQHDの240HzをDSCによって実現しているのだ。\n\n 加えて、WQHD 240Hz VRRパススルーに対応するAVerMediaの「Live Gamer ULTRA 2.1 - GC553G2」もElgatoの「4K X」も、これについては""DSCモードではない環境なら""という制約がある。\n\n EX270QMのサイトを見る限り、HDMIがDSC対応とは書かれておらず、DSCをオフにすることもできない。また、AVerMediaのサイトには「モニターのDSC機能を使用しないことをお勧めします」とのみ書かれていて、ElgatoのサイトにもDSCのことはほとんど書かれていない。今回、この検証を行なっていてキャプチャ経由でWQHD 240Hzのパススルー出力ができなかったため、AVerMediaとElgatoに問い合わせをして「モニターがDSCを使っていない」ことが必須だという事実を知ったのだ。\n\n では、どのモニターなら、これらのキャプチャユニットを使いながら、WQHDで240Hz出力ができるのか? AVerMediaに問い合わせたところLGの「27GR95QE-B」については動作確認が取れているとの返事があった。逆に、EX270QM以外にも、CORSAIRの「Xeneon 27QHD240」とGIGABYTEの「M28U」もDSCでの接続となるので、WQHD 240Hzパススルー出力はできないらしい。Elgatoのサイトも英語のサポートページにはDSC非対応の旨が記載されている。\n\n そこで、LGから27GR95QE-Bを借りて試したところ、GC553G2でも4K XでもWQHD 240Hzのパススルー出力ができた。ということで、WQHD 240Hzでのパススルーをしたいなら、おそらくこのモニターが唯一の選択肢となる。\n\n この環境は非常に快適だ。適切にG-SYNCを設定すれば、テアリングで画像が乱れたり、スタッタリングで描画がカクついたりすることはない。そして、ゲーミングPCと配信PC、キャプチャユニットの配線も必要最小限で済む。\n\n 本製品は、解像度がWQHDで240Hz対応のDisplayPortとHDMI×2を搭載し、色域がDCI-P3を98%カバーする点はEX270QMと同じ。大きな違いとして27GR95QE-Bはパネルが有機ELとなっている。\n\n 2台配信のメリットは、OBSなど配信ソフトの影響でゲーム性能が低下するとか、ゲームのせいでPCがクラッシュして配信が落ちるといった心配がない点。同じマシンでOBSで配信をしていると、ゲームによってはフレームレートが10%程落ちるものもある。普段の配信だけでなく、大会も配信したいというような高レベルなプレイヤーなら、2台配信は十分検討の価値がある。\n\n また、ブラウザを配信に映したらひょんなことから自分の本名が書かれたアカウント情報が表示されてしまった、あるいは、日本語入力システムの変換候補から自分の住所が表示されてしまったというセキュリティ/プライバシー問題についても、ゲーミングPCではブラウザにログインしない、日本語入力システムに学習をさせないといった対応を行なうことで防げるというメリットもある。\n\n■ 240Hzパススルーのまやかしと今回筆者が取った選択肢\n\n 実は、今回の検証を行なっている中で、4K Xについては、設定次第で""WQHD 240Hzのパススルー""ができることが分かった。設定次第というのは、独自ソフトの「4K Capture Utility」で「入力EDIDモード」を「結合済み」にする。ちなみにこれはデフォルトの設定だ。なので、最初筆者は、「4K Xなら俺の望みを叶えられるじゃん!」と思っていたのである。\n\n しかし、ここにも盲点は存在した。\n\n 確かに4K XとEX270QMの組み合わせで、Windows上ではWQHD解像度で240Hzを指定できる(かつHDRもオンにできる)。そして、NVIDIAコントロールパネルでは、G-SYNCを有効にもできる(HDMI 2.1のVRRはG-SYNC Compatibleという形でG-SYNCにも対応可能)。実際、NVIDIAコントロールパネルで「G-SYNCの互換性インジケータ」を有効にしてゲームを起動すると、「G-SYNC」の文字が表示されており、G-SYNCが機能しているように見える。\n\n だが、このG-SYNCはまやかしなのだ。実際にオーバーウォッチ2動かすと、フレームレートは230fps程度で安定しているのに、テアリングが発生するのだ。G-SYNCが機能している場合、GPUのフレームレートがモニターのリフレッシュレート(ここでは240Hz)に収まっていれば、テアリングは起きないはずだ。\n\n これは、垂直同期が取れていないことを意味する。実際、NVIDIAコントロールパネルを確認したところ、いろいろ検証している間に垂直同期がオンになっていなかった。\n\n G-SYNCを有効にするにはまず、NVIDIAコントロールパネルの「G-SYNCの設定」で「G-SYNC、G-SYNCとの互換性を有効化」および「選択したディスプレイモデルの設定を有効化」にチェックを入れる。\n\n 続いて、「3D設定の管理」の「グローバル設定」において、「モニターテクノロジ」が「G-SYNCとの互換性」(デフォルト)、「優先的に使用するリフレッシュレート」が「利用可能な最高値」(デフォルト)になっている上で、「垂直同期」を「3Dアプリケーション設定を使用する」(デフォルト)から「オン」に変更し、「最大フレームレート」を「オフ」からモニターリフレッシュレートより4ほど低い数値に変更する必要がある。G-SYNCを有効にしても、垂直同期がオフだと、フレームレートがモニターリフレッシュレートを超えたときテアリングが発生するためだ。\n\n ちなみに、本来は垂直同期をオンにするだけでいいはずなのだが、最大フレームレートがオフだと、フレームレートが天井に達した時、若干の入力遅延が発生する場合があるため、安全なマージンとして4フレームほど低い値に強制するといいと言われている。\n\n と言うことで、ひとまず垂直同期をオンにしてオーバーウォッチ2をプレイすると、テアリングは解消されたが、同時にフレームレートが140fps程度に落ち込んだ。何が起きたのか?\n\n 実は、4K XとEX270QMの組み合わせでデフォルト状態だと、システムはWQHDで240Hz表示できると""勘違い""しているのだ。しかし、4K X+EX270QMの環境において、WQHDで本当にパススルーできるのは、DSCを必要としない144Hzが上限なのだ。そのため、垂直同期をオフにして、240fpsでGPUから映像を送り出すと、4K Xのキャパシティ(144Hz)を超えたフレームレートとなるためテアリングが発生してしまうというわけだ。\n\n では、なぜ上記のような""勘違い""が発生するのか? これは、4K Xの入力EDIDモードによる。デフォルトの「結合済み」は、モニターとキャプチャのいいとこ取りをしたEDIDを設定する。これによってパススルー状態であっても、モニターが対応する240Hzが上限だと思ってしまうのだ。\n\n 入力EDIDモードにはこのほかに、「内部」と「ディスプレイ」がある。内部に設定すると240Hzも選択はできるのだが、モニターには映像が映らない。4K Capture Utilityでは取り込み画面が表示されているので、GPUから表示され取り込みまではできているのだが、パススルーができていない。144Hzでは問題なく表示される。「ディスプレイ」を選んだ場合は、そもそもOSで設定可能なリフレッシュレートの上限が120Hzとなる。\n\n なお、EX270QMにDisplayPortとHDMIを両方接続して、画面を複製させた場合でも、DisplayPort側では一見240Hzを利用できるように見えるのだが、G-SYNCを完全な形で有効にするとリフレッシュレートの上限は144Hzとなる。\n\n ちなみに、AVerMediaのユーティリティでもEDIDモードは変更できるのだが、この製品ではWQHD+EX270QMでの上限は常に144Hzとなる。\n\n と言うことで筆者が購入したEX270QMでは購入前に思い描いていた環境は実現できなかった。8年前に4K TVを買ったとき、HDMIの帯域が狭く、4K/HDRだと60fpsの入力ができないのが後から判明した""事件""を思い出してしまった。\n\n 失意に暮れながら筆者が選択したのは、144HzなVRR環境で利用するというものだ。テアリングを受け入れれば240Hz表示もできるが、これは個人的に受け入れがたい。モニターの最高性能を利用できないのは残念だが、144Hzと240Hzの体感差はかなり小さく、プロゲーマーであってもギリギリその違いを認識できるかどうかのレベルなので、諦めることにした。\n\n■ USBキャプチャの最高峰となるAVerMediaの「Live Gamer ULTRA 2.1」とElgatoの「4K X」\n\n 今回利用したAVerMediaの「Live Gamer ULTRA 2.1」とElgatoの「4K X」は、ここまでで紹介した通り、(モニターがDSCを利用しない場合)WQHDでの240Hzパススルー出力にも対応可能なUSBキャプチャユニットだ。細かいところでの仕様の違いはあるが、高解像度や高リフレッシュレートでキャプチャをしたい人にとっての最高スペックの製品と言っていい。それぞれの特徴を紹介しよう。\n\n Live Gamer ULTRA 2.1は、フルHDは360Hzまで、WQHDは240Hzまで、4Kは144Hzまでのパススルーに対応。いずれもHDRとVRRにも対応する。キャプチャ(録画)については、フルHDは240Hz SDRか120Hz HDR、WQHDは120Hz SDRか60Hz HDR、4Kは60Hz SDRか30Hz HDRまで対応する。これ以外にも、21:9となる、2,560×1,080ドットや3,440×1,440ドットにも対応できる。\n\n 特徴的な点として、3.5mmの音声端子が2つあり、キャプチャした音声の出力および、外部マイクの音声入力ができる。前面にカスタマイズ可能なRGB LEDを備えるのも特徴だ。\n\n 4K Xは、フルHDは240Hzまで、WQHDは240Hzまで、4Kは144Hzまでのパススルーに対応。こちらも、HDRとVRRにも対応する。キャプチャについては、フルHDは240Hz SDRか120Hz HDR、WQHDは144Hz SDRか60Hz HDR、4Kは144Hz SDRか60Hz HDRまで対応する。\n\n 特徴的な点として、OBSなどでゲーム画面とカメラ画像などを重ね合わせ配信に出力しつつ、キャプチャで取り込んだゲーム画面だけを別途同時に録画する機能もある。このほか、3.5mm音声入力端子もある。\n\n いずれの製品もUSBは10Gbpsの3.2 Gen2に対応している必要がある。また、いずれもUVCに対応しているので独自ドライバなどをインストールしなくても動作する。\n\n と言うことで、当初の目論見通り動かせなかったEX270QMだが、キャプチャをしないのであれば、HDMIについても制約はない。筆者のゲーミングPCはGeForce RTX 4070 SUPERを搭載しているが、このGPUはDSC接続にも対応する。そのため、キャプチャを行なわず、GPUとEX270QMをHDMI 2.1ケーブル対応ケーブルで直結するのであれば、WQHD 240HzでG-SYNCを利用できる。\n\n パネルはIPSで応答速度は1ms(中間色)、色域はDCI-P3を98%カバーし、DisplayHDR 600にも対応する。HDR対応ゲームを表示させたときの画質は、輝度も十分でメリハリのきいた色味で、画質の満足度は非常に高い。\n\n 加えて、リモコンが付属するのも、筆者のような2台環境では重宝する。ゲーミングPCはEX270QMにしかつないでいないが、配信PCは、配信以外に仕事でも使うので、こちらもEX270QMにつなぎ、計4画面で使っている。そのため、ゲームをプレイするときは、EX270QMを切り替える必要があるのだが、これを画面下にあるOSD操作ボタンでやるのは結構面倒。しかし、リモコンなら「入力切替ボタン」と、方向ボタンと決定ボタンを押すだけで、3秒で済む。\n\n DSCという罠はあったものの、トータルでは満足度の高い製品だ。\nPC Watch,若杉 紀彦",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000015-imppcw-000-2-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/3db87997f1cca7bb000facb8d7e6c7bddb25c88d/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1578/342/html/02_o.jpg.html']"
自分で撮影したグラビアを使い、Stable Diffusion用美女モデルを作成してみた（PC Watch）,https://news.yahoo.co.jp/articles/955bccd4de8c801187176f483fa9a302fb428038,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000013-imppcw-000-2-view.jpg?exp=10800,2024-03-25T06:15:18+09:00,2024-03-25T09:40:29+09:00,PC Watch,imppcw,PC Watch,8168,"\n写真：PC Watch\n日頃、civtaiなどにあるリアル系美女モデル(Model)と自作顔LoRAで遊んでいる筆者だが、自分で撮影したグラビアが直ぐ用意できるものだけでも数千枚あるので、これでモデル作ったらどうなる!? 的に始めてみた。\n【画像】TypeにCHECKPOINT MERGE\n■ MERGEとTRAINED、顔LoRAとファインチューニングの話\n\n civitaiのModelsを眺めていると、Typeに「CHECKPOINT MERGE」と「CHECKPOINT TRAINED」、2種類あるのに気づく。\n\n 前者は文字通り複数のモデルをマージして新しいモデルを作る手法。これは筆者もお気に入りのモデルを組み合わすとどうなるか? 的に試したことがある。簡単なやり方だが、AUTOMATIC1111に「Checkpoint Merger」という項目があるので、これを使う。\n\n 実際どんな動きをしているのか? までは把握していないので、ロジック的な説明はできないのだが、操作はモデルを2つ(以上)選び、Multiplier (M) - set to 0 to get model Aの数値を変更するだけといった感じで簡単だ。\n\n 上記は単純マージと呼ばれているものだが、階層マージと呼ばれる、各層に重みを付け、加重/差分マージを行なう高度な方法もある。そしてもうお気づきかと思うが、マージモデルは、画像を学習することはない。\n\n それ対して後者、TRAINEDは、名前の通り学習、つまり何らのテーマに沿った画像を用意し、それを基準となるモデルに学習させ、新しいモデルを作る手法だ。従って筆者がやりたいのはこちらとなる。\n\n 必要画像枚数が10～15枚程度と少ないものの、顔LoRA(Low-Rank-Adaptation)もざっくり言えばこの1種。基準となるモデルで学習し、学習した部分だけを別ファイルとして保存。必要に応じて使用する。モデル自体をファインチューニング(Fine Tuning)する方法と顔LoRAの違いを簡単にまとめると以下のようになる。\n\n 顔LoRAは、同一人物の顔さえ似ればいいので、枚数は極端に少なく、その分、learning_rateが桁2つ高くなっている。合計ステップ(step)数は10枚で2,000、15枚で3,000。これはどこかに基準が載っているわけではなく、筆者の経験から来た値だ。\n\n 対してファインチューニングは、同一人物ではなく、広く浅く(今回は日本人美女を)学習するため、できるだけ枚数は多く、そして顔LoRAより桁2つ少ないlearning_rateで学習する。いろいろ調べると、1万枚程度を使うとのこと。\n\n 問題は合計ステップ数。合計枚数=1 epochとなり、何epoch繰り返すのかだが、たとえば1万枚だと10回繰り返すだけで10万ステップという、顔LoRAとは桁違いの数字となる。\n\n これがどの程度が適正か? は、まだ始めたばかりなので不明な部分だったりする。\n\n■ SDXLリアル系モデルを作るにはどうすればよいのか\n\n 今回はSDXLのモデルをファインチューニングしたい。用意するものは以下の通り\n\n・テーマに沿った大量の画像\n・その1枚1枚にcaptionファイル(内容を説明した英文)\n・その1枚1枚にtagファイル(よくPromptでみかける 1girl, solo と言ったタグ)\n・sd-scripts\n・sd_xl_base_1.0.safetensors\n・GeForce RTX 3090や4090など、VRAM 24GBのGPU\n\n 1～5は後述するが、問題は6。実際学習すると分かるのだが、sd-scriptsの省VRAMオプションを駆使してもVRAMを約22GB使用する。従って24GB搭載しているRTX 3090、4090クラスでないとローカルPCでは学習できない。もちろんSD 1.5ならもっと少ない容量でOKだが、個人的に興味がないため試していない。\n\n■ テーマに沿った大量の画像\n\n 今回はすぐ出せる画像で手元に約4,500枚。もちろんまだまだあるのだが、古いものは着脱式HDD数十台に保存しており、さらにそのマウンタを装備するPCがなく(笑)、サクッと出せないので諦めた。\n\n 実は、そもそもこれをやってみようと思った理由なのだが、カメラマンでもない人が万単位の画像を集めるとなると、ネットからダウンロードするしか手がない(この是非ついては今回は触れない。あくまでも技術論としたい)。\n\n そして人物の場合、ほとんどが肌や顔を修正した画像となる。これで学習したものは、はたしてリアルな人肌? 人の顔? と言う疑問が前からあったのだ。\n\n 手持ちの画像は、筆者が撮影しており、納品先では修正していると思うが、手元のは無修正の写真。つまりリアルな人肌や顔だ。これを使って学習すればどうなるのか? が興味の対象だったりする。実際、顔LoRAも修正したり適当に撮った写真と、ちゃんと撮った無修正の写真では圧倒的な差が出ており、ファインチューニングでも同じでは? ということだ。\n\n 以下、Pythonのプログラムで処理している。実際はGeminiにこんな感じの仕様を提示しコードを出してもらった。\n\n・基準フォルダに画像が入った複数のフォルダごとコピー\n→ いきなり1つのフォルダに手動でコピーするとファイル名が重複するため\n・長辺1,024px未満は除外\n→ 対象は基準フォルダ以下全てのjpg|jpeg|JPG|JPEG\n→ SDXLは1,024x1,024だがオプション指定によりトリミングする必要無し\n・Adobe RGBのものはsRGBへ変換\n・アスペクト比を維持しつつ、長辺1,024pxへ縮小\n・連番のファイル名で出力フォルダへ保存。拡張子は.jpgへ統一\n\n 長くなるのでプログラムは省略するが、分かる人なら簡単に書けるし、分からない人でもGeminiやChat GPTなどに聞くと、似たようなコードが出てくる。\n\n これで1つのフォルダに長辺1,024pxで連番のファイル名となった画像ファイル(.jpg)が用意できた。\n\n■ .captionファイル\n\n いろいろ方法はあるのだが、BLIP-2を使用した。簡単に言えばマルチモーダルLLMの一種で画像からキャプションを生成する。モデルは、Salesforce/blip2-opt-2.7bと、Salesforce/blip2-opt-6.7bの2つあるのだが、後者は容量不足で動かなかった。\n\n これもPythonのプログラムで処理している。仕様は上記の画像を入れてあるフォルダから、1枚ずつ画像を読み込み、キャプションを生成。ファイル名は同じで拡張子を.captionとして保存する。\n\n 参考にしたのはこのサイト。基本、サイトにあるコードと同じなのだが、フォルダ内にある全画像でループを回し、.captionファイルを保存しているのが違いだ。初回は、モデルを自動的にダウンロードするので時間がかかる(blip2-opt-2.7bで15GBほど)。\n\nimage = Image.open(""../data/woman.jpg"")\n\n↓これを以下の様に変える\n#\n入力フォルダのパス\ninput_dir = ""/checkpoint/dataset""\noutput_dir = ""/checkpoint/caption""\n#\nサブフォルダも含めて画像ファイルを検索\nfiles = glob.glob(os.path.join(input_dir, ""**"", ""*.jpg""), recursive=True)\n\ncount = 1\n#\n画像ファイル処理\nfor file in files:\n#\nファイル名と拡張子を分割\nfilename, ext = os.path.splitext(os.path.basename(file))\n#\n画像を読み込み\nimage = Image.open(file)\n.\n. image = Image.open(""../data/woman.jpg"")以降のコードと同じ\n.\n#\n出力ファイル名\noutput_file = os.path.join(output_dir, filename + "".caption"")\n#\nキャプションをテキストファイルに出力\nwith open(output_file, ""w"") as f:\nf.write(generated_text)\n\ncount += 1\n\nprint(""処理が完了しました。"")\n\n 話は前後するが、先に後述するsd-scripts用の環境を作っておくと良い。と言うのも、CUDAなど、使用する基本的なモジュールは同じだからだ。その後、実行しモジュールがないと、止まった部分だけpip installすれば動くようになる。\n\n■ .txt(tag)ファイル\n\n これもいろいろあるが、JoyTagというのが目に止まったのでこれを使用。デモサイトがここにあり、このコードを参考にした。\n\n キャプションと動きは同じでループの中が変わっただけ。画像を入れてあるフォルダから、1枚ずつ画像を読み込み、tagを生成。ファイル名は同じで拡張子を.txtとして保存する。\n\n キャプションにしてもタグにしても、マルチモーダル対応のOpenAI APIなどを使えば可能なのだが、今回の写真のように、女性で肌露出が多い画像を入力すると、利用規約違反で動かないので、あてにしない方がいい(笑)。\n\n■ sd-scriptsで学習\n\n さてこれでデータセットに必要な全データが揃ったので、sd-scriptsを実行する環境を用意する。今回のファイル構造は以下の通り。もちろん適当に変えても問題はない。\n\nファイル構造\ncheckpoint/\n├dataset/\n├output/\n├config.toml\n├meta_clean.json\n├sd_xl_base_1.0.safetensors\nsd-scripts/\n└finetune/\n\n まずMinicondaでpython 3.10.6の環境を作り切り替える。次にsd-scriptsをgit clone、モジュールをインストールする。この辺りはGitHubにある手順と同じだが、bitsandbytesとtritonを追加でインストールする必要がある。accelerate configの最後はbf16を指定。\n\nconda create -n sds python=3.10.6\nconda activate sds\n\ngit clone https://github.com/kohya-ss/sd-scripts sd-scripts\ncd sd-scripts\n\npip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --index-url https://download.pytorch.org/whl/cu118\npip install --upgrade -r requirements.txt\npip install xformers==0.0.20\npip install bitsandbytes\npip install .\triton-2.0.0-cp310-cp310-win_amd64.whl\n※ https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/triton-2.0.0-cp310-cp310-win_amd64.whl を事前にダウンロード\n\naccelerate config\n- This machine\n- No distributed training\n- NO\n- NO\n- NO\n- all\n- bf16\n\n 次にファインチューニングでは、.captionファイルと.txtファイルを1つの.jsonファイルへまとめる必要があり、sd-scriptsのfinetuneフォルダにあるプログラムを利用する。実行する前に、.captionファイルと、.txtファイルを画像のあるdatasetにコピーするのを忘れずに!\n\n.caption から .json変換\npython merge_captions_to_metadata.py --full_path '/checkpoint/dataset' meta_cap.json\n\n.txtから .jsonへ変換/追記\npython merge_dd_tags_to_metadata.py --full_path '/checkpoint/dataset' --in_json meta_cap.json meta_cap_dd.json\n\nクリーニング(不要なものを削除する)\npython clean_captions_and_tags.py meta_cap_dd.json meta_clean.json\n\n これらを順に実行するとmeta_clean.jsonができているので、これを /checkpointへコピー。config.tomlで指定する\n\nconfig.toml\n[[datasets]]\nresolution = [1024, 1024]\nbatch_size = 1\n[[datasets.subsets]]\nimage_dir = '/checkpoint/dataset'\nmetadata_file = '/checkpoint/meta_clean.json'\n#\n上記で変換したjson\n\n これで準備OK! sd-scriptsフォルダに戻り以下のコマンドを実行すれば学習を開始する。\n\naccelerate launch --num_cpu_threads_per_process 1 sdxl_train.py \\n--pretrained_model_name_or_path='/checkpoint/sd_xl_base_1.0.safetensors' \\n--dataset_config='/checkpoint/config.toml' \\n--output_dir='/checkpoint/output' \\n--output_name='MyCkpt' \\n--save_model_as=safetensors \\n--max_train_epochs=100 \\n--learning_rate=5e-6 \\n--save_every_n_epochs=5 \\n--use_8bit_adam \\n--xformers \\n--mixed_precision='bf16' --full_bf16 \\n--gradient_checkpointing \\n--enable_bucket --bucket_no_upscale \\n--no_half_vae \\n--cache_text_encoder_outputs\n\n sd_xl_base_1.0.safetensorsは、Stableが出しているSDXLのベースモデル。これを使って学習する。というのも、美女は用意した画像に大量に入っているものの、一般的な風景や物などは全く入っておらず、これまで用意するのは大変だからだ(というか無理)。そこは、このモデルを使う、というわけだ。ほかにお気に入りのモデルがあればそれでも良い。\n\n いろいろパラメータがあるが、意味はこんな感じとなる。\n\n・--enable_bucket --bucket_no_upscale : 縦位置/横位置異なるサイズ対応。1,024x1,024/1:1にトリミングする必要なし\n・--cache_text_encoder_outputs : テキストエンコーダーをキャッシュ(GPU負荷低減)\n・--mixed_precision='bf16' --full_bf16 : 必ず併用。fp16だとVRAMが足らず動かない\n・--save_every_n_epochs=5 : 5 epochs毎に保存\n・--max_train_epochs=100 : 最大epoch数\n・--learning_rate=5e-6 : 通常1e-6～5e-6。どの辺りが適正値か不明\n\n 学習が始まり、かかる時間を見ると、ざっくり4.5K枚1周で1時間(GeForce RTX 3090 + eGPUボックス)。おそらくPCI Express x16接続のGeForce RTX 4090だと倍の速度で処理できる=4.5K枚30分。オプションで指定した--max_train_epochs=100だと、1 epoch = 4,500ステップ×100なので450,000ステップ。100時間=約4日! かかる計算だ。なおこの100が正解なのか、もっと少なくていいのか、もっと多く必要なのかは不明。\n\n また、learning_rateも5e-6が正解なのだろうか。1e-6/2e-6/3e-6/4e-6どれがいいのかも分からない。順に試すにしても、結果が出るまで時間がかかる。とりあえず真ん中の3e-6を試したが、5e-6より効きが弱い。よって4e-6か5e-6、どちらかとなりそうな感じだろうか。\n\n 執筆中はまだ30 epochs。この状態でのXYZプロットと、同一設定のsd_xl_base_1.0とMyCkptで生成した画像を掲載した。5～15 epochsだと、体が崩れるケースも多かったのだが、20 epochs以降、それが減った感じだ(なくなったわけではない)。\n\n 今の課題は、顔はともかく体が崩れやすいこと。epoch数なのか? そもそもそlearning_rate=5e-6が高いのか? 3e-6では少しの変化だったので、3e-6か4e-6で20 epochs程度まで回せば分かると思われるが、約20時間……。結果が分かるまで時間がかかり過ぎる(笑)。\n\n 入稿後、SDXLのデフォルト--learning_rate=は4e-7と知り、試したところ10 epochsでは変化は少ないものの、体が崩れる現象はそれほど出ず、顔も良好だった。さらに回すとどうなるか!? これから試される方は、4e-7で始めてほしい。\n\n また、全ての画像、著作権は筆者だが、モデル用に学習してもいいのか、各所属事務所に確認していない関係上、できたモデルをcivitaiなどで公開することはない。あくまでも筆者専用実験モデルだ。\n\n 以上、sd-scriptsを使ってファインチューニングする方法を解説した。LoRA関連は調べれば山盛り出るのだが、ファインチューニングに関しては部分的に書かれたものはあっても、全てまとまっているのは皆無。主にsd-scriptsのGitHubにある情報だけを頼りに試している。従って見落としなど、間違っている/抜けている部分もあるかもしれないが、その時は何らかの方法でお伝えしたい。\nPC Watch,西川 和久",['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20240325-00000013-imppcw-000-2-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/955bccd4de8c801187176f483fa9a302fb428038/images/000', 'https://pc.watch.impress.co.jp/img/pcw/docs/1577/939/html/T01_o.jpg.html', 'https://news.yahoo.co.jp/search?p=%23&source=article-body']"
