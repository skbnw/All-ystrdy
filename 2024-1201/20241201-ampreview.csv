headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
MITスピンオフ企業が開発した「リクイッド・ファウンデーション・モデル」、トランスフォーマーに依存しない新アーキテクチャで既存モデルを凌駕（AMP［アンプ］）,https://news.yahoo.co.jp/articles/bf0bfa12ac3c7e8c0ad4c66bdd116f5c22208e42,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241201-00010000-ampreview-000-1-view.jpg?exp=10800,2024-12-01T06:00:28+09:00,2024-12-01T06:00:28+09:00,AMP［アンプ］,ampreview,AMP［アンプ］,1248,リクイッド・ファウンデーション・モデル（LFM）とは、その概要 「リクイッド・ファウンデーション・モデル」 マサチューセッツ工科大学（MIT）のコンピュータ科学・人工知能研究所（CSAIL）の元研究者によって設立されたLiquid AIが、新たなマルチモーダルAIモデル「リクイッド・ファウンデーション・モデル（LFM）」を発表した。LFMの最大の特徴は、2017年の論文「Attention Is All You Need」で提唱されたトランスフォーマーアーキテクチャに依存しない点にある。 Liquid AIは、「エンジニアが自動車や飛行機を設計したのと同じように、基本原理から設計する」という理念を掲げており、実際にそれを実現した格好となる。現在、LFMは3つのサイズとバリエーションで展開されている。最小の「LFM 1.3B」、中規模の「LFM 3B」、そしてMistralのMixtralに似た「Mixture-of-Experts」モデルである「LFM 40B MoE」の3種類。 モデル名の「B」は10億（billion）を表し、モデルの情報処理、分析、出力生成を制御するパラメータ数を示している。一般的に、パラメータ数が多いモデルほど、より広範なタスクをこなすことが可能となる。 LFMと主要モデルの規模別パフォーマンス比較（Liquid AIウェブサイトより） https://www.liquid.ai/liquid-foundation-models Liquid AIによると、LFM 1.3Bは、科学、技術、工学、数学（STEM）分野にわたる57の問題で構成される人気ベンチマーク「Massive Multitask Language Understanding（MMLU）」において、メタの新しいLlama 3.2-1.2BやマイクロソフトのPhi-1.5を上回るパフォーマンスを示した。10億パラメータ規模のモデルとして、非GPTアーキテクチャがトランスフォーマーベースのモデルを大きく上回った初の事例となる。 3つのモデルは、いずれもメモリ効率を最適化するように設計されている。Liquid AIのポストトレーニング責任者であるマキシム・ラボン氏は、自身のXアカウントで、LFMの主な利点として、大幅に少ないメモリ使用量でトランスフォーマーベースのモデルを上回るパフォーマンスを発揮する点を強調している。 これらのモデルは、ベンチマークテストだけでなく、実際の運用面でも競争力を持つように設計されており、金融サービス、バイオテクノロジー、家電製品など、エンタープライズレベルのアプリケーションからエッジデバイスへの展開まで、幅広いユースケースに対応できる。ただし、これらのモデルはオープンソースではなく、ユーザーはLiquidのインファレンスプレイグラウンド、Lambda Chat、またはPerplexity AIを通じてアクセスする必要がある点に留意が必要だ。,[],[]
