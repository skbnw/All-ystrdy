headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
NTTテクノクロス、パスキー認証が簡単に導入できる「PASUTTO」を提供開始へ（ビジネス＋IT）,https://news.yahoo.co.jp/articles/a6ad6676858ecc5afc8c89447d34509be274a7c6,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00149845-biz_plus-000-1-view.jpg?exp=10800,2024-10-04T12:50:05+09:00,2024-10-04T12:50:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,270,\n（写真：ビジネス+IT）\nNTTテクノクロスは10月3日、デバイスの指紋認証や顔認証を使ったパスキー認証サービス「PASUTTO」を10月9日から販売すると発表した。\n\n このサービスは、Webサイトやモバイルアプリへの簡単なパスキー認証の導入を目的としており、既存システムにスクリプトを埋め込むだけで設定できる。価格は1サイトあたり初期費用20万円、月額50万円からという。\n\n パスワード管理の負担を減らし、UX向上が期待されるパスキー認証の普及が進む中で、NTTテクノクロスはさらにこの分野の成長を支援する構えだという。,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00149845-biz_plus-000-1-view.jpg?pri=l&w=640&h=360&exp=10800'],['https://news.yahoo.co.jp/articles/a6ad6676858ecc5afc8c89447d34509be274a7c6/images/000']
「あいまい」「なあなあ」はヤバすぎ、組織を伸ばす「バックオフィス」の目標設定（ビジネス＋IT）,https://news.yahoo.co.jp/articles/cf96207f1dd3328fd30ebbbc166f0b9ef7de5723,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00149490-biz_plus-000-1-view.jpg?exp=10800,2024-10-04T07:10:05+09:00,2024-10-04T07:10:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,2890,\nバックオフィスの目標設定のコツとは？次ページでは、今すぐ使える「部門別の目標例」も紹介します（Photo/Shutterstock.com）\n総務や人事、経理、労務、広報といったバックオフィス部門の評価基準があいまいな会社は珍しくありません。バックオフィス部門は、営業部門などのように成果が数字に分かりやすく表れるわけではないため、目標設定が難しいと考えられていますが、それは誤解です。ここでは、伸びる会社が実践している、バックオフィスの目標設定と評価制度について解説します。\n【詳細な図や写真】同じ業務量でも「効率的で定時退社する人」よりも「非効率的で残業して対応する人」のほうが評価されていたら危ない……（Photo/Shutterstock.com）\n「あいまい」な評価制度がもたらす弊害\n業種や業態を問わず、バックオフィス部門では「積極的に仕事に取り組む」や「協調性を大切にする」といったあいまいな目標が設定されがちで、上司の好き嫌いや何となくの印象に基づく評価が横行しています。\n\n たとえば、同じ仕事をするなら、短い時間で効率良く行うほうが優秀とされるべきなのに、非効率的で遅くまで残業をして終わらす社員が「頑張っている」として評価されることは往々にしてあるでしょう。\n\n\n このような状況では、社員には早く仕事を終わらせようとする動機が働きません。それに、明確な評価制度がないと何を優先すべきかが不明確になります。あれもこれもやらねばと思い、無駄なストレスを抱えてしまいますし、上司に嫌われて評価が落ちないかを気にして仕事に集中できないことも考えられます。\n\n 優秀な人ほど、ここで努力しても意味はないと感じてモチベーションを失い、離職を決断してしまうでしょう。いずれにせよ、会社にとって大きな損失です。\n目標のポイントは、評価時に「主観が入る余地がない」内容\n「明確な評価制度」が必要と述べましたが、もう少し具体的に言えば、「いつまでに何をすればどういう評価を得られ、収入がどの程度になるか」がはっきりと分かる仕組みです。\n\n 評価する側とされる側の主観が一切入らない「定量的な目標」を設定しましょう。目標を立てる期間は、1カ月単位だと短過ぎ、6カ月単位だと長過ぎるため、「3カ月（四半期）に1度」がベストです。\n\n ルーティンワークの多いバックオフィス部門だと、明確な評価指標を作れないと考えられていますが、そんなことはありません。次ページでは、部門別（総務、広報、マーケティング、人事）に具体的な目標の例を出しながら説明します。\n【すぐ使える】総務／広報／人事の「定量的な目標」の具体例\nたとえば、総務担当なら下記のような形が考えられます。\n\n\n・すべての顧客情報の入力を契約後1週間以内にミスなく完了させる\n・業務改善提案を行い、上長の承認を1件得る\n・タスク期限順守率90％を達成する\n\n 3つ目の「タスク期限順守率」とは、唐突に生まれる仕事の対応が多い総務を評価するのにとても便利な指標です。\n\n たとえば、「この見積書を今日の16時までに作成して提出してください」という依頼に対し、しっかりと期限を守れたかで判断します。もちろん、予定があって難しければその事情を説明し、依頼者が再設定する期日に間に合えば可とします。\n\n\n 広報・マーケティング担当なら、下記のような指標にします。\n\n\n・自社ホームページ・メディアのPV数60万を達成する\n・Webを通じた集客を60件達成する\n・チラシを通じた集客を60件達成する\n\n 人事であれば、次の通りです。なお、採用する人材の質については、「自社で用意する適性検査で一定以上のスコアを獲得する」といった条件設定をしておきます。\n\n\n・新卒採用者を3人獲得する\n・中途採用者を3人獲得する\n・インターンに10人動員する\n\n ここまで紹介した評価指標はいずれもメンバー層ですが、たとえば総務部の管理職であれば、下記のような具合です。\n\n\n・経常利益5％達成する\n・部下5人のうち4人が目標を達成する\n目標が設定できたら…「平等に評価」する合理的な方法\n上記で紹介した目標はあくまで例です。もし、これから評価制度を構築しようとするのであれば、最初の1～2カ月を使って業務の洗い出しをすると良いでしょう。各部門・役職の役割を定義するほか、日次、週次、月次で発生する業務も整理しておきます。\n\n それを基に3カ月単位での定量的な目標を定め、それぞれに0～100点の基準点を設けてください。ただし、合格は60点とします。\n\n 「すべての顧客情報の入力を契約後1週間以内にミスなく完了させる」であれば、ミス2件までは許容範囲内として60点とし、ミス1件で80点、ミス0件で100点、ミス3件なら40点、ミス4件は20点、ミス5件で0点とします。\n\n 「自社ホームページ・メディアのPV数60万を達成する」なら、60万PV達成で60点合格。70万PVで70点、80万PVなら80点のように階段を踏んでいく形にします。\n\n あとは、合計で何点以上取れば合格なのかを決め、獲得した点数に合わせて給与が変わる号俸を用意すれば完了です。そのうえで重要なのは、給与が下がる仕組みにしておき、毎度同じ目標を達成すればそれで良しとはせず、少しずつ目標は高くしていくことです。こうしないと、現状維持は良いことだと社員が錯覚してしまいかねません。\n\n 給与の減少という危機感こそが社員の奮起を促します。もちろん、目標を超え続ければ給与が継続して増えるようにすべきです。\n目標達成するためにリーダーが欠かしてはいけないこと\n評価制度が明確になると、社員は上司からどう思われているかを過剰に気にする必要がなくなります。そして、頑張ったぶん収入が増える仕組みがあれば、社員は安心して業務に取り組める環境が整い、結果として仕事の質が向上するでしょう。\n\n とはいえ、社員が必ずしも毎回好成績を収められるわけではありません。あなたが部下を持つ立場であれば、なかなか成績が振るわない部下がいたとき、怒鳴るような真似をするのではなく、目標に到達できるよう導きましょう。\n\n 具体的には、3カ月に1度の目標から毎週のKPI（重要業績評価指標）を出し、その進捗状況を週に1度報告させるのです。問題がなければ何もなしで良いですが、進捗が芳しくなければ、励ましたり詰ったりするのではなく、ひと言「どうしますか」とだけ聞いてください。現状の打開策を部下自身に考えさせることで成長につながります。\n\n 社長や上層部の一存でバックオフィス部門の評価を決めている会社は、見方を変えれば大きなチャンスがあるとも言えます。ぜひ、本記事を参考にして明確な評価制度の構築に乗り出してください。\n執筆：識学 シニアコンサルタント 橋本 潤也,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00149490-biz_plus-000-1-view.jpg?pri=l&w=640&h=426&exp=10800'],"['https://news.yahoo.co.jp/articles/cf96207f1dd3328fd30ebbbc166f0b9ef7de5723/images/000', 'https://www.sbbit.jp/article/cont1/149490#image188215']"
なぜ「令和の米騒動」は起こった？ “元凶”農政に欠けすぎている「ある視点」とは（ビジネス＋IT）,https://news.yahoo.co.jp/articles/a0049d712b25cfa895dc3524a80fb1264ef94c3a,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00146532-biz_plus-000-1-view.jpg?exp=10800,2024-10-04T06:50:05+09:00,2024-10-04T06:50:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,3731,\n惣菜市場推移（2021年－2023年）（出典：一般社団法人日本惣菜協会「2024年版惣菜白書 拡大編集版 －ダイジェスト版－」）\nスーパーからコメが消えた「令和の米騒動」、その影響は計り知れない。値ごろなコメを必要とする「食品業界」が直撃を受け、特にごはんを使う惣菜や冷凍食品で成長を遂げてきた「中食産業」は、まさに大打撃だ。この危機に業界はどのように立ち向かうのか。騒動の根本的な原因と現在の業界のリアルについて、一般社団法人 日本惣菜協会 専務理事の清水 誠三氏と、一般社団法人 日本冷凍食品協会 専務理事の出倉 功一氏に聞いた。\n【詳細な図や写真】惣菜、冷凍食品などの需要は過去3年で好調。しかし現在は伸びる需要に冷や水となっている（写真：筆者提供）\n需要が増え続ける、おにぎり、弁当などの「中食業界」\nコメづくりは需要が減り続ける斜陽産業だと思われてきた。ところが、そんな不景気な話をどこ吹く風と、コメの需要を増やし続けている分野がある。中食業界だ。\n\n「家庭内で炊飯する機会はどんどん減少しています。そのために食の外部化が進み、中食需要が増加し、自然、この分野でコメの消費が拡大しています。スーパー、コンビニエンスストア、惣菜専門店での弁当やおにぎりの売り上げは上昇しています」（清水氏）\n\n こう話すのは、一般社団法人 日本惣菜協会 専務理事の清水 誠三氏。惣菜の市場規模は、11兆円に迫ろうとしている。コロナ禍の始まった2020年こそ前年比でマイナスとなったものの、21年以降は右肩上がりだ（上図）。その成長率は外食や家庭内食を上回る。\n\n\n 弁当やおにぎり、すしといった米飯類も例外ではなく、過去3年にわたってプラス成長を続ける。レッドオーシャンと化しつつあるコメ業界が、この潜在需要に応えない手はない……はずなのだが、現実は必ずしもそうなっていない。\nブランド米戦国時代と業務用米不足のカラクリ\n中食が必要とするのはいわゆる業務用米で、比較的安価なコメ。北海道の「きらら397」に代表される値ごろ感のあるB銘柄に需要があるのだが、こうしたコメは農家と行政から概して不人気である。どうせなら、コシヒカリに代表される家庭での炊飯を前提としたブランド米を作って高く売りたい。これが農業側にありがちな発想だ。\n\n 道府県によるブランド米の開発と発売は、途切れることがない。岩手の「銀河のしずく」、宮城の「だて正夢」、秋田の「サキホコレ」、山形の「つや姫」、「雪若丸」、新潟の「新之助」、福井の「いちほまれ」、富山の「富富富（ふふふ）」……。\n\n デビューしたてのころは、テレビCMや広告による露出が多く、スーパーの棚にも並ぶ。ただ、ブランド米が氾濫し戦国時代のような様相を呈しているので、一握りしか棚には残れない。多くは淘汰（とうた）され、消費者から忘れられていく。\n\n ブランド米の開発に血道を上げる。その不毛さは、コメの消費の移り変わり（下図）を見れば一目瞭然だ。家庭での炊飯は、コロナ禍で在宅時間が増えた「巣ごもり需要」による一時的な揺り戻しを例外として、減り続けている。\n\n 消費が低迷するブランド米を漫然と作り続ける。このことが業務用米の不足を招いてきた。\n\n\n 清水氏はこう嘆いていた。\n業界関係者が嘆く、農政に欠けすぎている「ある視点」\n「コメの消費を拡大するためには、伸長市場に手を打つ、マーケットインの発想が必要。従来からコメ政策はプロダクトアウトの政策であり、変化への対応ができていない。伸長する中食産業が使用する業務用米に向けた政策が必要なのです。業務用米の不足、価格の上昇という現状では、伸長市場のメニューが、ごはんの盛りを減らす、寿司のシャリを小さくするなど、むしろ消費増にブレーキをかける状況になっています」（清水氏）\n\n 農水省は米価を高く保つことにきゅうきゅうとしている。そのために、コメの生産を抑制するいわゆる減反政策を1970年以降、続けてきた。コメの需要は毎年10万トンのペースで減っている。こう決めてかかりこそすれ、需要を伸ばす可能性を検討してこなかった。\nなぜスーパーからコメが消えたのか？コメ政策の「誤算」\n「コメの需要は減り続ける」という前提が誤っていたことが、今夏のコメ不足で露呈した。農水省は毎年、「適正生産量」を算出し、生産量がこれを超えないよう都道府県に促す。2024年産のそれを前年並みの669万トンに設定していた。\n\n ところが、農家の高齢化や生産意欲の減退で作付けが減り、猛暑で収量が下がったため、現実の生産量は661万トンまで落ち込んでしまう。かたや需要量は702万トンと、農水省の予想を30万トン超上回った。結果として、41万トンが不足し、スーパーの棚にコメがない事態になったというのだ。\n\n 需要量が増えた要因として、物価高騰でコメに値ごろ感が出たことが指摘されている。特にコムギの価格が上がり、麺やパンが値上がりする中、価格の上昇幅が小さいコメが例年より多く購入された。さらに訪日外国人が増えたことも影響した。8月に南海トラフ地震臨時情報が出され、備蓄用の買いだめまで起きた。\n\n コメには需要を伸ばせる余地がある。減反政策で生産量を抑えることこそが、その可能性をつぶしてしまっている。\n\n「消費の多いところにコメを供給して米離れをなくし、生産量を増やしていこう。普通はこう考えるじゃないですか。それをまったく違うところに力を入れて、どんどん消費を減退させてきたというのが、今のコメ政策じゃないか」（清水氏）\n中食業界が被った「大打撃」\n2023年産米の8月の相対取引価格は、平均で60キロ当たり1万6133円だった。新米が出回り始める時期に入りながら、前年産のコメがこんな高値を付けるのは異常だ。スポット取引価格といって、スポットでコメを手当てする売買価格は、優に2万円を超えていた。\n\n 米飯類は惣菜の43.9％を占めるだけに、業界の被った打撃は大きい。\n\n「夏前から価格の上昇が続き、スポット価格は倍近くまで上昇しました。また契約を結んでいた企業も売り上げが順調で、契約量をオーバーした分は相当高い価格でないと入手できない状況となりました。協会としても農水省に備蓄米の放出を要請したが、緊急時でないと出せないとの回答」（清水氏）\n\n 清水氏は、「米不足が騒がれている折に、あるいは自治体の首長から備蓄米放出の要請をした折に、備蓄米の放出があるかもしれないとの口先介入だけでもあれば、市場は冷静さを保ったかもしれない」と残念がる。\n「チャーハン戦争」で需要の増す冷凍大手の動向\n冷凍食品もまた、成長著しい商材である。生産量は下のグラフの通り、増加傾向にある。2023年がやや減ったのは、コロナ禍による巣ごもり需要で消費が増えた分の反動と見られる。\n\n\n かつて「手抜き」と捉えられがちだった冷凍食品は、いまや料理の手間を省ける「手間抜き」の便利な食品として、日常的に食べられている。若年層ほど利用の頻度が高く、今後も成長が見込める。\n\n 米飯は凍結後の調理を経ても、品質が損なわれず、冷凍に適した素材と言える。生産量が多いのは、チャーハン、ピラフ類、おにぎり。これらは、冷凍食品の中で国内の生産量上位20品目にランクインしている。\n\n ほかにも、赤飯やまぜご飯、ちらしずし、ドリア、ガパオライスなど、さまざまな種類の商品が生まれている。業務用だと、単なる白ごはんや、介護施設でも使えるおかゆもある。\n\n 一般社団法人 日本冷凍食品協会 専務理事 出倉 功一氏は、こう話す。\n\n「単身世帯が増えるだけでなく、お子さんのいる世帯であっても1人で食べる（個食）機会が増えています。冷凍食品でも、ワンプレートにごはんとおかずをまとめた、一食完結型のお弁当に近い商品もあります」（出倉氏）\n\n\n 冷凍米飯の市場を拡大させるきっかけとなったのが、2015年ごろに話題となった「冷凍チャーハン戦争」、「チャーハン祭り」と呼ばれる、メーカー各社による開発競争だった。\n\n 「冷凍米飯の中でもチャーハン系は大きく伸びている」と出倉さん。米飯を使う冷凍食品の製造量は、おにぎり2万7000トン、ピラフ類4万8000トンに対し、チャーハン8万8000トン（いずれも2023年）と圧倒的に多い。\n\n\n 冷凍米飯の原料は基本的に国産米だ。概して、重量当たりの単価が安いものが好まれる。各メーカーは、商品に求められる品質特性と価格を見極めながら、原料米を調達する。\n\n「冷凍食品大手メーカーはコメの使用量が多く、今夏は基本的に必要量を確保できていた。大手メーカーに関しては、今夏のコメ不足が、生産と販売に大きく影響するほどではなかったようです。とはいえ、今後の原料供給や価格がどうなるのか。メーカーは関心を寄せています」（出倉氏）\n執筆：ジャーナリスト 山口 亮子,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00146532-biz_plus-000-1-view.jpg?exp=10800'],"['https://news.yahoo.co.jp/articles/a0049d712b25cfa895dc3524a80fb1264ef94c3a/images/000', 'https://www.sbbit.jp/article/st/146532#image188062']"
グーグル「Gemma」は何がすごい？試してわかる小型言語モデルで「GPT-3.5超え」の実力（ビジネス＋IT）,https://news.yahoo.co.jp/articles/a1d02d7c7a4ba848259addcf4bc197827cc6b642,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00149481-biz_plus-000-2-view.jpg?exp=10800,2024-10-04T06:10:05+09:00,2024-10-04T14:45:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,5259,\nグーグル「Gemma 2」はなぜ小型なのにハイパフォーマンスなのか？（出典：グーグル）\nオープンソース言語モデルや小型言語モデル（SLM）の精度は従来、それほど高いものではなく、実際に活用することは難しかった。しかし、現在はローカル環境でも十分に利用できる小型モデルが続々登場しており、その様相は大きく変わってきている。グーグルが7月末にリリースした「Gemma 2」は、その好例といえるだろう。20億パラメータという非常に小さなモデルでありながら、GPT-3.5を超えるパフォーマンスを示したのだ。さらに、グーグルは「日本語版 Gemma 2 2B」を発表、高い日本語性能を発揮したという。パラメータ数に関わらず、Gemma 2が高いパフォーマンスを発揮できる理由はどこにあるのか、その理由を探りつつ、実際にローカル環境で動作させる方法と必要なマシンスペックについてもわかりやすく解説する。\n【詳細な図や写真】Phi-3 miniはMMLUで68.8％を獲得したものの課題も多かった（出典：Phi-3 Technical Report）\n小型モデルの飛躍的な進化、最近の動向\n大規模言語モデル（LLM）の進化は、パラメータ数の増大とともに進んできた。たとえば、OpenAIのGPT-3は1750億パラメータを持ち、その後継モデルGPT-4に至っては、パラメータ数が1兆を超えると推定されている。こうした巨大モデルの運用には、膨大な計算リソースが必要となり、そのコストは天文学的な数字に上る。\n\n しかし、最近の技術革新により、パラメータ数が数億から100億以下の小型言語モデルが目覚ましい進化を遂げている。これらのモデルは、低コストかつローカル環境でも運用可能でありながら、高いクオリティを実現している。\n\n マイクロソフトが2024年4月に発表した「Phi-3」は、その代表例だ。\n\n Phi-3の最小モデル「Phi-3-mini」は、わずか38億パラメータでありながら、法学、数学、哲学、薬学などの幅広い分野の知識を問うベンチマークテスト「MMLU」で68.8%を獲得。OpenAIのGPT-3.5（71.3％）に迫る数字を記録した。さらに、算数問題の解決能力を測るGSM-8Kでは82.5％を達成し、GPT-3.5の78.1％を上回った。\n\n\n メタも小型モデルの開発に注力している。同社が2024年6月末に発表した研究論文では、パラメータ数が数億という超小型モデル「MobileLLM」が紹介された。MobileLLMは125M（1億2500万パラメータ）と350M（3億5000万パラメータ）の2つのバリエーションが開発されており、スマートフォンでの利用に最適化されている。\n\n メタの研究者らによると、3億5000万パラメータのMobileLLMが必要とするメモリは約350MBに抑えられており、これは一般的なスマートフォンのDRAM容量の5%以下に収まるという。\nグーグルのGemma 2 2B、GPT-3.5を超え、その実力\nグーグルも小型モデル開発で主導権を握りたい考えだ。同社が2024年7月にリリースした「Gemma 2」は、そのコミットメントの現れと見て取ることができる。\n\n Gemma 2は、2B（20億パラメータ）、9B（90億パラメータ）、27B（270億パラメータ）からなる比較的小型の言語モデルファミリーだ。特にGemma 2 2Bの効率性が際立っており、注目を集めている。\n\n AI研究グループLMSYSの独立評価において、Gemma 2 2Bは1126点を獲得。Mixtral-8x7B（1114点）やGPT-3.5-Turbo-0314（1106点）を上回る結果となったのだ。この評価は、LMSYSが運営するChatbot Arenaと呼ばれるプラットフォームで実施されたもの。\n\n\n 同プラットフォームのリーダーボードによると、2024年8月6日時点では、合計127のモデルが評価対象となっており、総投票数は161万507票に達する。\n\n リーダーボードの詳細を見ると、Gemma 2 2Bは52位にランクイン。51位のClaude-2.0（1132点）や、58位のGPT-3.5-Turbo-0613（1117点）と拮抗するパフォーマンスを示している。\n\n 特筆すべきは、（執筆時点では）Gemma 2 2Bが7197票という比較的少ない投票数でこのスコアを達成していることだ。一方、GPT-3.5-Turbo-0613は38935票と、はるかに多くの評価を受けている。\n\n Chatbot Arenaの評価は、人間の選好に基づくペアワイズ比較を採用している。これは、2つのモデルの回答を並べて表示し、ユーザーがより良いと感じた方に投票するという方式だ。この手法により、モデルの実際の使用感に近い評価が可能となっている。\n\n Gemma 2 2Bの成功は、AIモデルの大きさが必ずしもパフォーマンスに直結しないことを示唆するもの。高度なトレーニング技術、効率的なアーキテクチャ、高品質なデータセットの組み合わせにより、パラメータ数の少なさを補完できることが証明された格好となる。\n\n Gemma 2 2Bはオープンソースで公開されており、研究者や開発者がHugging Faceを通じてアクセスできる。これにより、AIコミュニティ全体での知見の共有と、さらなる改良が期待される。\n小型モデルの進化の詳細\nGemma 2 2Bが小型ながら高いパフォーマンスを発揮する要因として、トレーニング技術の進化、効率的なアーキテクチャの採用、高品質データセットの使用が挙げられる。これらの要素について詳しく見ていきたい。\n\n まず、トレーニング技術の進化について。Gemma 2の開発では、知識蒸留（Knowledge Distillation）という手法が採用された。グーグルのテクニカルレポートによると、この手法は、大規模モデル（教師モデル）の出力確率分布を小規模なモデル（生徒モデル）に学習させるものだ。具体的には、教師モデルが与える各トークンの確率分布と、生徒モデルの出力する確率分布の差を最小化するように学習を進める。\n\n この手法の利点は、単純な次トークン予測よりも豊富な情報を学習できる点にある。\n\n たとえば、教師モデルが「猫」と「犬」という選択肢に対して、60%と40%の確率を出力した場合、生徒モデルはこの微妙な確率の差も学習することができる。これにより、限られたパラメータ数でも、より深い言語理解が可能となる。\n\n 効率的なアーキテクチャの採用も、Gemma 2の性能向上に貢献している。同レポートによると、Grouped-Query Attention（GQA）と呼ばれる手法が採用された。これは、人工知能が情報を処理する際の「注意」の仕組みを効率的にグループ分けする手法だ。\n\n 人間でたとえると、複数の人が同時に異なる部分を見て情報を集める作業を、より少ない人数で効率的に行うようなもの。この工夫により、さまざまな課題での性能を維持しつつ、AIの処理速度が向上したという。\n\n さらに、「局所的な注意」と「全体的な注意」を交互に行う設計を採用。これは、人間が文章を読むときに、一部分に集中して読みつつ、時々全体を見渡すような読み方に似ている。この方法により、文章の細部と全体の関係を理解しつつ、処理速度も向上させることに成功した。\n\n たとえば、長い物語を理解する場合、AIは1つの段落を詳しく読み（局所的な注意）、次に物語全体の流れを確認する（全体的な注意）といった具合に処理を行う。これにより、物語の細かい描写と全体的なプロットの両方を効率よく理解できるようになる。\n\n 高品質データセットの使用も重要な要素だ。\n\n Gemma 2 2Bは、2兆トークンという膨大なデータセットでトレーニングされた。このデータセットは、ウェブ文書、コード、科学記事など、多様なソースから構成されている。さらに、データの品質を保つため、厳密なフィルタリングが行われた。\n\n 具体的には、望ましくない表現や不適切なコンテンツの除去、個人情報の削除、評価セットの汚染防止などが実施されたとのこと。\n\n これらの要素が相互に作用し合うことで、Gemma 2 2Bは小型ながら高い性能を実現。特に知識蒸留の採用により、パラメータ数の制約を超えた言語理解が可能となった点が大きい。今後、こうした技術の進化により、より小型で効率的なモデルの開発が加速することが期待される。\n小型言語モデルをローカル端末で運用するためのスペック／コスト\nここでは、小型モデルをローカルマシンで運用するにはどれほどのスペック／コストが必要なのか、その概算をみていきたい。\n\n まずGemma 2 2Bの16ビット版に必要なメモリは、5.4GBになる。一般的に、LLMのトレーニングでは、32ビットが使用されるが、推論ではスピードやコストの観点から16ビットが使用されることが多い。Gemma 2 2Bの16ビット版とは、グーグルがリリースしたオリジナルのGemma 2 2Bに最も近い性能を維持するモデルといえる。\n\n AIコミュニティでは、低スペックPCでも利用できるように、量子化が実施され、8ビットや4ビットなどに縮小されたバージョンも複数提供されている。たとえば、Gemma 2 2Bの4ビット版の必須メモリは2～4GBほどに縮小される。\n\n Gemma 2 2B・16ビット版の場合、RTX3050など6GBのVRAMを持つ3～4万円クラスのGPUを搭載したPCでも問題なく利用できる可能性がある。PC全体もおそらく15万円以内で購入することができるはずだ。\n\n 量子化版であれば、Gemma 2の9Bモデル（90億パラメータ）など、比較的大きなモデルも利用することが可能だ。Gemma 2・9Bモデルは、4ビット版で7.3GBのメモリを必要とする。\n\n GPUメモリの使用に関するGitHubのOllamaリポジトリの報告によると、Llama3.1の70Bモデル（700億パラメータ）を運用する際、24GBのGPUメモリのうち21.1GBが使用されたという。さらに、モデル全体で39.3GBのメモリが必要とされ、不足分は18.2GBの共有GPUメモリ（実質的にはシステムRAM）が使用されたことが報告されている。\n\n この事例から、GPT-3.5と同等以上の性能を持つ8B～9Bクラスのモデル（量子化版）であれば、12GB程度のメモリを搭載したGPUでも十分に運用可能だと推測される。具体的には、NVIDIA RTX3060やRTX4060といったミドルレンジのGPUが候補となる。\n\n 同じユーザーによると、CPUに関しては、Intel Core i7-13700KF（16コア、24スレッド）が使用されたとされる。このCPUでLlama3.1 70Bモデルを運用した際、CPU使用率は約50%だった。8B～9Bクラスのモデルであれば、より低スペックのCPUでも十分に対応可能だと考えられる。たとえば、Intel Core i5-12600KやAMD Ryzen 5 5600Xクラスのプロセッサで十分かもしれない。\n\n RAMに関しては、モデルサイズの2～3倍程度が望ましい。8B～9Bクラスのモデルであれば、32GBのRAMがあれば十分だと考えられる。ストレージについては、NVMe SSDが推奨される。モデルのロード時間を短縮するためだ。256GB程度あれば、複数のモデルをインストールしても余裕がある。\n\n これらの条件を満たすPCのカスタマイズ例は以下のようになる。\n\n\nCPU：Intel Core i5-12600K\nGPU：NVIDIA RTX 3060 12GB\nRAM：32GB\nストレージ：NVMe SSD（256GB）\n\n これに、OSライセンス費用、マザーボード、電源ユニット、ケース、冷却ユニットなどを加えると13万～17万円ほどで揃えることができるだろう。この構成であれば、GPT-3.5と同等以上の性能を持つLLMをローカル環境で運用できるはずだ。\n\n ただし、Gemma 2・9Bモデルに関して、16ビット版では、19GBメモリが必須となるため、24GBを持つRTX4090などハイエンドGPUが必要になることには留意したい。\n執筆：細谷 元、構成：ビジネス＋IT編集部,['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241004-00149481-biz_plus-000-2-view.jpg?pri=l&w=640&h=374&exp=10800'],"['https://news.yahoo.co.jp/articles/a1d02d7c7a4ba848259addcf4bc197827cc6b642/images/000', 'https://www.sbbit.jp/article/cont1/149481#image187906']"
