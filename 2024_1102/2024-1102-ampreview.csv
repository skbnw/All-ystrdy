headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
AIモデル開発／ファインチューニングで起こり得るGPUのメモリ不足を解消するアプローチ（AMP［アンプ］）,https://news.yahoo.co.jp/articles/fda75cf2464e6f3e696976de4837d172a523ff19,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241102-00010000-ampreview-000-1-view.jpg?exp=10800,2024-11-02T06:03:21+09:00,2024-11-02T06:03:21+09:00,AMP［アンプ］,ampreview,AMP［アンプ］,4115,"\nAIモデル開発／ファインチューニングのコスト\nAIモデル開発／ファインチューニングのコスト\n大規模言語モデル（LLM）の開発やファインチューニングには、莫大な計算リソースが必要となる。その中でも特に、GPUのコストが大きな課題となっている。\n\nLLMの開発コストは、モデルの規模によって大きく異なる。たとえば、グーグルのGemini Ultraは開発に1億9,100万ドル（約268億円）、OpenAIのGPT-4は7,800万ドル（約110億円）かかったと推定されている。比較的小規模なモデルでも、DatabricksのDBRXは1,000万ドル（約14億円）の開発コストがかかったと報告されている。\n\nこれらの巨額な開発コストは、ほとんどの組織にとって手の届かないものだ。そのため、多くの企業は既存のオープンソースモデルをベースに、自社のニーズに合わせてファインチューニングを行う方法を選択している。\n\nしかし、ファインチューニングにもかなりのコストがかかる。特に、パラメータ数の多いモデルのファインチューニングは非常に高コストだ。SmartCatのAIエンジニア、ミロス・ジビック氏の分析によると、700億パラメータを持つLlama 3モデルのファインチューニングには約1.5テラバイトのGPU VRAMが必要となる。これは20台のNVIDIA A100（各80GB VRAM）に相当し、このような構成のGPUコストは約40万ドル（約5,620万円）にも上る。\n\nクラウドプロバイダーを利用する場合でも、コストは決して安くない。AWSの8台のA100 GPUを1時間使用するコストは約40ドル（約5,620円）だ。700億パラメータモデルを20台のGPUで5日間ファインチューニングすると、約1万2,000ドル（約158万円）のコストがかかる計算になる。\n\nこれらのコスト課題により、実際の現場では100億パラメータ未満の比較的小規模なLLMを主に使用するケースが多いとジビック氏は述べている。これらのモデルは、16GBから24GBのVRAMで訓練可能であり、より手頃な価格で実装できる。たとえば、Mistral 7B（70億パラメータ）モデルをセルビア語用にファインチューニングする場合、AWSのNVIDIA A10インスタンスを使用して10時間未満、20ドル（約2,800円）以下のコストで実現可能だという。\nファインチューニングのコストを下げる方法\nAIモデル開発やファインチューニングにおけるコスト課題に対し、ソフトウェア面での最適化により、限られたリソースでも効率的にAIモデルを訓練する方法が注目を集めている。\n\nその代表的な手法の1つが「混合精度訓練（Mixed precision training）」だ。これは、低精度のb/float16演算と標準的なfloat32演算を組み合わせることで、計算速度とメモリ使用効率の向上を目指す手法。ここで登場する「b/float16」や「float32」は、コンピュータが数値を表現する方法を指す。「float32」は32ビットの浮動小数点数で、より精密な計算が可能だが、メモリを多く使用する。一方、「b/float16」は16ビットで表現するため、精度は若干落ちるものの、メモリ使用量は半分で済み、計算も高速化できる。この方法により、AIモデルの訓練時間を短縮しつつ、使用するGPUメモリも削減できる。このアプローチは、特に大規模な言語モデルの訓練やファインチューニングで効果を発揮する。モデルの大部分を低精度で計算しつつ、精度が重要な部分のみ高精度で計算することで、性能を落とすことなく効率化を図れるのだ。\n\nもう1つの重要な手法が「アクティベーションチェックポイント」である。これは必要最小限の値のみを保存し、他は必要に応じて再計算する方法で、メモリ使用量を最大70%削減できる。「アクティベーション」とは、ニューラルネットワークの各層で生成される中間的な計算結果のことを指す。通常、これらの結果はすべてメモリに保存されるが、大規模なモデルではこれが膨大なメモリを消費する原因となる。アクティベーションチェックポイントは、全ての中間計算結果を保存せず、必要な時に再計算する方法だ。長い数式を解く時、全ての途中経過を書き留めるのではなく、重要なポイントだけメモを取って後で必要な部分を計算し直すようなイメージといえる。\n\n具体的には、ネットワークの特定の層（チェックポイント）でのみ結果を保存し、それ以外の層の結果は必要に応じて再計算する。これにより、メモリ使用量を大幅に削減できるが、一部の計算を繰り返す必要があるため、計算時間が若干増加する傾向がある。しかし、多くの場合、メモリ使用量の削減によって得られるメリット（より大きなバッチサイズでの訓練や、より大規模なモデルの使用が可能になるなど）が、計算時間の増加を上回る。特に、GPUのメモリが限られている環境では、この手法が非常に有効になる。\n\nさらに、複数のGPUを同時に使用する「マルチGPU訓練」も効果的だ。大きな仕事を複数の人で分担して行うように、計算作業を複数のGPUで分担して処理する。これにより、従来の手法と比較して最大10倍以上の高速化が可能となる。\n\nこれらの基本的な最適化手法に加え、より高度な技術も開発されている。たとえば「カーネルフュージョン」は、複数の小さな計算作業を一つの大きな作業にまとめる方法だ。また「動的パディング」は、データの長さを効率的に調整する方法で、GPUのメモリ使用量を下げることが可能となる。「適応的学習率スケジューリング」も重要な技術の1つ。これは学習の進み具合に応じて、学習速度（学習率）を自動的に調整する方法だ。\n\nこれらの最適化技術を組み合わせることで、限られたリソースでも効率的にAIモデルのファインチューニングを行うことが可能となる。特に、中小企業やスタートアップにとっては、これらの技術を活用することで、高額なGPUリソースを必要とせずにAIモデル開発／ファインチューニングに取り組める可能性が広がっている。\nコストを下げる各種ツール、Unslothの強み\nUnslothウェブサイト （https://unsloth.ai/）\n上記のような手法を実行するには、専門的な知識が必要となるが、最近では、ファインチューニングコストの削減に特化したツール／プラットフォームが続々登場しており、それらを活用することで、比較的簡単かつ低コストでファインチューニングすることができるようになっている。\n\n中でも注目を集めているのが「Unsloth」だ。Unslothは、LLaMAアーキテクチャをベースとした大規模言語モデル（LLM）のファインチューニングを最適化／高速化するためのオープンソースツールキットだ。\n\nUnslothの最大の特徴は、消費者向けハードウェアでも効率的にLLMのファインチューニングが可能な点だ。開発者によると、従来の手法と比較して最大5倍の速度向上を実現し、メモリ使用量も大幅に削減できるという。特筆すべきは、これらの最適化は自動的に適用されるため、ユーザーが個別に実装する必要がない点にある。\n\n具体的な性能としては、AlpacaデータセットでのファインチューニングにおいてHugging Faceの1倍に対し1.98倍、Flash Attentionの1.04倍に対し1.98倍の速度を達成。これは業界標準と言えるHugging Faceのライブラリと比較して、Unslothは約2倍の速度でファインチューニングを実行できるということ。また、高速化技術として知られるFlash Attentionと比較しても、Unslothは約1.9倍の速度を実現している点も特筆に値する。Flash Attentionが既に高速な処理を行っていることを考えると、非常に印象的な数値と言えるだろう。\n\nさらに、メモリ使用量はHugging Faceの1万8,235MBに対し9,631MBと、47.18%の削減を実現。これは、一般的に入手しやすい16GBのVRAM（メモリ）を搭載したGPUを使う場合、Hugging Faceのアプローチではメモリが不足してしまうが、Unslothを使用すれば余裕を持って処理を行えることを示している。または、同じハードウェアでより大規模なモデルや大きなバッチサイズを扱えるようになることも示唆される。\n\nUnslothは、混合精度訓練やアクティベーションチェックポイント、マルチGPU訓練といった基本的な最適化技術に加え、カーネルフュージョンや動的パディング、適応的な学習率スケジューリングなどの高度な技術も実装している。これらの技術を組み合わせることで、大幅な性能向上を実現しているのだ。\n\n使いやすさも重視されており、Google Colabなどのノートブック環境で簡単に利用可能だ。また、Hugging FaceのTransformersライブラリとの統合も進んでおり、既存のコードベースに容易に組み込むことができる。\n\nUnslothと類似したツールとしては、マイクロソフトのDeepSpeed、Hugging FaceのAccelerateなどがある。\n\nUnslothはローカル環境にインストールして使用できるため、プライバシーやセキュリティを重視する企業にとっても適している。センシティブなデータやプロプライエタリなモデルを扱う場合に有用だ。\n\nこのようなツールの登場により、中小企業やスタートアップ、個人開発者でも、効率的にLLMのファインチューニングに取り組めるようになった。AIモデル開発の民主化が進み、より多様なアプリケーションの創出につながることが期待される。\n文：細谷元（Livit）","['https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241102-00010000-ampreview-000-1-view.jpg?pri=l&w=640&h=360&exp=10800', 'https://newsatcl-pctr.c.yimg.jp/t/amd-img/20241102-00010000-ampreview-001-1-view.jpg?pri=l&w=640&h=312&exp=10800']","['https://news.yahoo.co.jp/articles/fda75cf2464e6f3e696976de4837d172a523ff19/images/000', 'https://news.yahoo.co.jp/articles/fda75cf2464e6f3e696976de4837d172a523ff19/images/001']"
